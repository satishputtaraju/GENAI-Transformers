{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888dcbdc",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2: Transformer Architecture Exercise\n",
    "\n",
    "This notebook serves as a reference implementation for **Assignment 2** of the generative AI course.  The goal is to compare three prominent transformer architectures—**decoder‑only**, **encoder‑only**, and **encoder‑decoder**—on a common generative task.  The assignment requires training each architecture on the same dataset, evaluating their performance with common metrics, and analysing the implications of architectural differences on generative tasks and chain‑of‑thought reasoning.\n",
    "\n",
    "## Dataset selection\n",
    "\n",
    "For this exercise we use the **CNN/DailyMail** summarisation dataset (version `3.0.0`) from Hugging Face’s `datasets` library.  The dataset comprises news articles paired with human‑written summaries; each article–summary pair provides a natural input/output example for a generative model.  Because the data are already split into training/validation/test splits and are widely used for abstractive summarisation research, this dataset is appropriate for comparing generative architectures.  Although `WikiText` could be used for language modelling tasks, summarisation requires models to generate structured output given an input, which better illustrates differences between decoder‑only, encoder‑only, and encoder‑decoder designs.  For compute efficiency in this notebook we subsample the dataset (e.g. a few hundred training examples) rather than using the full corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb4b15",
   "metadata": {},
   "source": [
    "\n",
    "## Overview of transformer architectures\n",
    "\n",
    "We train three different transformer models:\n",
    "\n",
    "* **Decoder‑only (GPT‑style):** These models consist of stacked self‑attention blocks in which each token can attend only to previous tokens (causal masking).  We use `GPT‑2` as the base model and fine‑tune it to generate a summary from an article.  Because GPT‑2 is a pure language model, we construct input prompts of the form `\"summarize: <article>\"` and train the model to predict the target summary.  During training we mask out the prompt part of the input so that the loss is computed only on the summary tokens.\n",
    "\n",
    "* **Encoder‑only (BERT‑style):** Encoder‑only models such as `BERT` learn bi‑directional contextual representations using masked language modelling (MLM).  They are not inherently generative; they excel at understanding tasks (e.g. classification, token classification).  For a fair comparison on generative tasks we fine‑tune BERT on the same corpus using MLM, combining article and summary text into a single sequence.  At evaluation time we assess perplexity and use the `fill‑mask` capability to approximate generation.  This highlights BERT’s limitations on tasks requiring free‑form generation.\n",
    "\n",
    "* **Encoder‑decoder (T5‑style):** Models like `T5` encode the input sequence with an encoder and decode the output sequence with a separate decoder.  They can perform a wide range of text‑to‑text tasks, including summarisation and question answering.  We fine‑tune `T5‑small` on the CNN/DailyMail dataset using the standard prefix `\"summarize: \"` in the input to indicate the task.  During evaluation we compute ROUGE metrics on generated summaries.\n",
    "\n",
    "The following sections implement data loading, preprocessing, model fine‑tuning, and evaluation for each architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030bd9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3.12.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "numpy       : 2.3.3\n",
      "matplotlib  : 3.10.6\n",
      "scikit-learn: 1.7.2\n",
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Using Python {sys.version.split()[0]}\")\n",
    "\n",
    "# Install required packages into the current notebook environment\n",
    "%pip install -qU numpy matplotlib scikit-learn\n",
    "\n",
    "# Verify versions\n",
    "import numpy as np, matplotlib, sklearn\n",
    "print(\"numpy       :\", np.__version__)\n",
    "print(\"matplotlib  :\", matplotlib.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"✅ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d888988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers evaluate\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542eddf5",
   "metadata": {},
   "source": [
    "\n",
    "## Load and inspect the dataset\n",
    "\n",
    "We load the CNN/DailyMail dataset using the Hugging Face `datasets` library.  To accelerate training for demonstration purposes we take a small subset of the training and validation sets (e.g. 500 training examples and 100 validation examples).  Each record contains two fields:\n",
    "\n",
    "* `\"article\"`: the news article text (input).\n",
    "* `\"highlights\"`: the human‑written summary (target).\n",
    "\n",
    "Below we load the dataset, inspect a few examples, and create the smaller subsets used for fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c7d3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': '(CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to my clinic with frequent cold symptoms and infections. One way to see if her immune system functioned properly was to check her response to childhood vaccines. But because the girl\\'s mother had decided against vaccinating her, I could not perform the vaccine blood test. My hands were tied behind my back; I had no way of knowing if there was a deficiency in her immune system. And if she did have a deficiency, she had no protection from the potentially deadly diseases the vaccines are designed to inoculate. As it turned out, she didn\\'t have a life-threatening infection -- this time. Her parents and I breathed a sigh of relief. We can\\'t know for sure that the anti-vaccine movement, led by uninformed celebrities and discredited research, is the reason we are having such a terrible measles outbreak in the United States. But certainly it\\'s not helping our children. Before measles vaccinations were available, every year in the United States, approximately 500,000 people were infected with measles and 500 died. That number fell to around 60 infections annually after vaccinations were introduced in 1994. The Vaccines for Children program, implemented in 1994, helped lead to the eradication of measles by the year 2000. The Centers for Disease Control and Prevention estimates that in the last 20 years, vaccinations prevented 322 million illnesses and 732,000 deaths, while saving $295 billion. Now, we can add measles to the list of illnesses that are crawling back out of the dustbin of history. Cases of measles have been on the rise since 2010, and globally, 14 people die from measles every hour. The CDC reports the majority of those who have contracted measles were not vaccinated. Some of the outbreaks have been from unvaccinated U.S. citizens getting measles abroad and bringing it back to those not vaccinated in the United States. The trend of not receiving vaccinations has been fueled by many, including Jenny McCarthy. The son of the former co-host of \"The View\" was diagnosed with autism in 2007. Since then she has been fervently critical of vaccination, claiming vaccines such as those for measles, mumps and rubella cause autism. She founded an organization for the cause, wrote books on the topic and made countless appearances -- all the while arguing, absent any scientific data, that vaccines cause autism. McCarthy\\'s views have been influential in fanning parents\\' fears of vaccinating their children. She backtracked her stance on vaccines in an op-ed she wrote in April 2014, claiming she is not \"anti-vaccine,\" but the damage has been done. Another harmful influence was former British surgeon and researcher Andrew Wakefield. In 1998, he presented a highly publicized Lancet report connecting autism and vaccines. Later, the study was discredited by many sources. In 2010, the British General Medical Council declared his paper linking vaccine and autism was not only based on bad science, but also a deliberate fraud. For his unethical behavior he lost his medical license. But his fraudulent work has contributed to a decrease in vaccinations and increase in measles. The only credible association between autism and vaccine is age. When a child is diagnosed with a developmental delay such as autism, parents naturally search for a cause and look to recent events in the child\\'s life. Since the age of onset of autism is similar to when children receive vaccinations, some parents falsely connect these two unrelated events. Autism, in fact, appears to be rising among unvaccinated children as well. While parents have a right to be skeptical and well-informed when it comes to their children\\'s health, they must not make medical decisions based on unfounded diatribes from a celebrity or fraudulent medical paper. 5 things you don\\'t know about measles . Measles outbreaks in our own backyard should prompt educated discussion with patient families who are not vaccinated. We have many credible studies demonstrating that the advantages of vaccination outweigh the disadvantages. Vaccines reduce the threat of disease or death by a particular pathogen. Unvaccinated children face challenges when it comes to getting lifesaving care. Every 911 call, doctor appointment, or emergency room visit, you must alert medical personnel of your child\\'s vaccination status so he receives distinctive treatment. Because unvaccinated children can require treatment that is out of the ordinary, medical staff may be less experienced with the procedures required to appropriately treat your unvaccinated child. As was the case with the 4-year-old patient I saw, doctors are not be able to adequately assess a patient\\'s health if the patient is not vaccinated. During pregnancy, women who are not vaccinated can be vulnerable to diseases that may complicate their pregnancy. If an unvaccinated pregnant woman contracts rubella, her baby may have congenital rubella syndrome, causing heart defects, developmental delays and deafness. Choosing to not vaccinate your child not only puts your own child at risk, but it also puts others at risk if their child isn\\'t protected. Some people cannot be vaccinated because of other medical conditions, such as cancer. These people rely on the general public being vaccinated so their risk of exposure is reduced. According to CDC statistics comparing pre-vaccine era deaths to now, there has been a 99% decrease in measles. I know I will continue to give my own children vaccinations to protect their life and health. I urge everyone to do the same for the sake of all children.', 'highlights': 'The U.S. is experiencing the worst measles outbreak in 20 years .\\nJennifer Shih: The anti-vaccine movement can be blamed for scaring parents .\\nShe says widespread vaccination was responsible for eradicating measles years ago .\\nShih: Choosing to not vaccinate your child is bad for your child and other children .', 'id': 'a592df3cff8b44b5bb7222f638a0e6de402f9534'}\n",
      "Example validation record: {'article': '(CNN)\"A picture of horror.\" That\\'s how German Foreign Minister Frank-Walter Steinmeier described the site where a Germanwings Airbus A320 plane crashed in the French Alps on Tuesday. \"The grief of the families and loved ones is immeasurable,\" Steinmeier said, after flying over the area in the Alps in southeastern France. \"We must stand with them. We are all united in great grief.\" Departure: Barcelona, Spain, at 10:01 a.m. (26 minutes late) Destination: Scheduled to land in Dusseldorf, Germany, at 11:39 a.m. Passengers: 150 (144 passengers, six crew members) Airplane: Airbus A320 (twin-jet) Airline: Germanwings (budget airline owned by Lufthansa) Flight distance: 726 miles . Last known tracking data: 10:38 a.m. Last known speed: 480 mph . Last known altitude: 11,400 feet . Last known location: Near Digne-les-Bains, France, in the Alps . Sources: CNN and flightaware.com . Flight 9525 took off just after 10 a.m. Tuesday from Barcelona, Spain, for Dusseldorf, Germany, with 144 passengers -- among them two babies -- and six crew members. It went down at 10:53 a.m. (5:53 a.m. ET) in a remote area near Digne-les-Bains in the Alpes de Haute Provence region. All aboard are presumed dead. Helicopter crews found the airliner in pieces, none of them bigger than a small car, and human remains strewn for several hundred meters, according to Gilbert Sauvan, a high-level official in the Alpes de Haute Provence region who is being briefed on the operation. Authorities were not able to retrieve any bodies Tuesday, with the frozen ground complicating the effort. Wednesday may not be much easier, with snow in the forecast. Spanish and German officials moved to join hundreds of French firefighters and police in the area, working together to help in the recovery effort and try to figure out exactly what happened. As of Tuesday evening, there were few clues. One of the aircraft\\'s data recorders, the so-called black boxes, has been found, according to French Interior Minister Bernard Cazeneuve, but it was too early to tell what it would say about the crash. \"We don\\'t know much about the flight and the crash yet,\" German Chancellor Angela Merkel said. \"And we don\\'t know the cause.\" Relatives of those believed to be on the flight, fearing the worst, gathered at the Barcelona airport, where a crisis center was set up. French authorities set up a chapel near the crash site. Lufthansa Group said the company will look after the relatives of those on board. \"There will be a contact center established in France; relatives who would like to take advantage of this will be transferred to the contact center at no cost -- and their accommodation paid for -- just as soon as the center has been established,\" Lufthansa said. Those aboard included a \"high number of Spaniards, Germans and Turks,\" according to Spain\\'s King Felipe VI. Germanwings CEO Thomas Winkelmann said it\\'s believed 67 people, or nearly half those on the plane, are German citizens. Germanwings crash: Who was on the plane? Sixteen students and two teachers from one German high school, called Joseph Koenig Gymnasium, were among those booked on Flight 9525, according to Florian Adamik, a municipal official in Haltern, the town where the school is located. A crisis center has been established at the city hall in Haltern, which is about 77 kilometers (48 miles) north of Dusseldorf\\'s airport. Winkelmann confirmed the 16 students and two teachers were on the plane. Haltern\\'s mayor, Bodo Klimpel, said they had been heading home after taking part in a foreign exchange program. \"The whole city is shocked, and we can feel it everywhere,\" Klimpel said. A Dutch citizen and a Belgian -- the latter a resident of Barcelona -- were among those on the flight, according to those countries\\' foreign ministries. Two Australians and two Colombians were also believed to be on board. Germanwings started in 2002 and was taken over by Lufthansa seven years later as its low-cost airline, handling an increasing number of midrange flights around Europe. It was forced to cancel some flights Tuesday because there were crews that didn\\'t want to fly upon hearing news of the crash. The valley where the plane went down is long and snow-covered, and access is difficult, said the mayor of the nearby town of Barcelonnette, Pierre Martin-Charpenel. It was well populated in the 19th century but there are almost no people living there now, he said. It\\'s an out-of-the-way place with magnificent scenery, he said. The sports hall of a local school has been freed up to take in bodies of the victims of the plane crash, said Sandrine Julien from the town hall of Seyne-les-Alpes village. Seyne-les-Alpes is about 10 kilometers from the crash site. Mountain guide Yvan Theaudin told BFMTV the crash was in the area of the Massif des Trois Eveches, where there are peaks of nearly 3,000 meters (1.9 miles). It\\'s very snowy in the area and the weather is worsening, he said, which could complicate search and rescue efforts. Responders may have to use skis to reach the crash site on the ground, he said. Sandrine Boisse, president of the tourism office at the Pra Loup ski resort, said she heard the plane crash and called the police and the local government office to find out what had happened. \"It was about 11 (a.m.) here. I was outside the garage, and we heard a strange noise, and at first we thought it was an avalanche,\" she said. \"Something was wrong. ... We didn\\'t know what.\" A mountain guide who heard a plane fly at alarmingly low altitude shortly before the crash, Michel Suhubiette, said helicopters may be the only way to get to the crash site. According to the U.S. Federal Aviation Administration, just under 16% of aviation accidents occur during the cruise portion of a flight -- meaning after the climb and before descent. Accidents are more common during takeoff and landing. The twin-engine Airbus A320s, which entered service in 1988, is generally considered among the most reliable aircraft, aviation analyst David Soucie said. The captain of the crashed plane had flown for Germanwings for more than 10 years, and had more than 6,000 flight hours on this model of Airbus. The plane itself dates to 1991 and was last checked in Dusseldorf on Monday, according to Winkelmann. So what happened? CNN aviation analyst Mary Schiavo said the plane\\'s speed is one clue. According to Germanwings, the plane reached its cruising altitude of 38,000 feet, and then dropped for eight minutes. The plane lost contact with French radar at a height of approximately 6,000 feet. Then it crashed. This could indicate that there was not a stall, but that the pilot was still controlling the plane to some extent, Schiavo said. Had there been an engine stall, the plane would have crashed in a matter of minutes, she said. That small piece of information about the descent means that the pilot could have been trying to make an emergency landing, or that the plane was gliding with the pilot\\'s guidance, Schiavo said. A scenario where the plane was gliding is potentially more dangerous because wide fields for landing would be hard to come by in the mountains, she said. The crash spurred officials in several countries to offer their condolences and pledge solidarity and cooperation to help those affected and determine what happened. \"Our thoughts and our prayers are with our friends in Europe, especially the people of Germany and Spain, following the terrible airplane crash in France,\" U.S. President Barack Obama told reporters. \"It\\'s particularly heartbreaking because it apparently includes the loss of so many children, some of them infants.\" Germany\\'s Merkel said she was sending two ministers to France on Tuesday and would travel to the crash site on Wednesday to see it for herself. \"We have to think of the victims and their families and their friends,\" she said. German Foreign Minister Frank-Walter Steinmeier said the German government had set up a crisis center in response to the \"terrible news\" and was in close contact with the French authorities. \"In these difficult hours, our thoughts are with those who have to fear that their close ones are among the passengers and crew,\" he said. CNN\\'s Mariano Castillo, Hala Gorani, Laura Akhoun, Stephanie Halasz, Lindsay Isaac, Josh Levs, Richard Greene, Karl Penhaul and Sara Delgrossi contributed to this report.', 'highlights': 'The plane reached 38,000 feet, and then dropped for eight minutes, Germanwings says .\\nVictims from Germany, France, Spain, Turkey, Belgium, Holland, Colombia, Australia .\\nOne data recorder found from Germanwings plane that crashed in Alps .', 'id': '95ecfc3b12bb6bd0bee29ea2b52519634ac4279e'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=50).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=50).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n",
    "print(\"Example validation record:\", small_val_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab968dd",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "I was trying with Varying Train Size and Val Size and found that the Max Train Size is 287113 and Val Size is 13368. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9c398",
   "metadata": {},
   "source": [
    "\n",
    "## Decoder‑only model: GPT‑2 fine‑tuning\n",
    "\n",
    "A decoder‑only transformer must learn to generate a summary given an input article.  We use a prompt‑based approach: the input text has the form `\"summarize: <article>\"`, and the model is trained to produce the summary tokens.  To prevent the model from learning to predict the prompt tokens, we mask the loss on the prompt portion of the sequence (by setting corresponding labels to `-100`).\n",
    "\n",
    "We use the `GPT‑2` tokenizer and model from Hugging Face.  Because GPT‑2 lacks a padding token by default, we add a pad token equal to the end‑of‑text token.  We then tokenize the inputs and construct labels accordingly.  The function below performs these steps and is mapped over the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a6c9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:01<00:00, 409.49 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 715.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "478ce2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 808.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96aaa",
   "metadata": {},
   "source": [
    "#### The above run was executed after I had changed the train size and value size to the max. I see the number of Examples processed per second in this case has increased drastically compared to the previous run with train size 500  and val size 100. Also the execution took more time (4 mins and 20 Seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c576f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 287113/287113 [02:27<00:00, 1945.64 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:08<00:00, 1618.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=100, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "319162db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 287113/287113 [02:58<00:00, 1608.06 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:11<00:00, 1123.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to my clinic with frequent cold symptoms and infections. One way to see if her immune system functioned properly was to check her response to childhood vaccines. But because the girl's mother had decided against vaccinating her, I could not perform the vaccine blood test. My hands were tied behind my back; I had no way of knowing if there was a deficiency in her immune system. And if she did have a deficiency, she had no protection from the potentially deadly diseases the vaccines are designed to inoculate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=200, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7683098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 287113/287113 [02:46<00:00, 1727.32 examples/s]\n",
      "Map: 100%|██████████| 13368/13368 [00:08<00:00, 1647.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to my clinic with frequent cold symptoms and infections. One way to see if her immune system functioned properly was to check her response to childhood vaccines. But because the girl's mother had decided against vaccinating her, I could not perform the vaccine blood test. My hands were tied behind my back; I had no way of knowing if there was a deficiency in her immune system. And if she did have a deficiency, she had no protection from the potentially deadly diseases the vaccines are designed to inoculate. As it turned out, she didn't have a life-threatening infection -- this time. Her parents and I breathed a sigh of relief. We can't know for sure that the anti-vaccine movement, led by uninformed celebrities and discredited research, is the reason we are having such a terrible measles outbreak in the United States. But certainly it's not helping our children. Before measles vaccinations were available, every year in the United States, approximately 500,000 people were infected with measles and 500 died. That number fell to around 60 infections annually after vaccinations were introduced in 1994. The Vaccines for Children program, implemented in 1994, helped lead to the eradication of measles by the year 2000. The Centers for Disease Control and Prevention estimates that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=350, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d57e4145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized GPT-2 input:\n",
      "summarize: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to my clinic with frequent cold symptoms and infections. One way to see if her immune system functioned properly was to check her response to childhood vaccines. But because the girl's mother had decided against vaccinating her, I could not perform the vaccine blood test. My hands were tied behind my back; I had no way of knowing if there was a deficiency in her immune system. And if she did have a deficiency, she had no protection from the potentially deadly diseases the vaccines are designed to inoculate. As it turned out, she didn't have a life-threatening infection -- this time. Her parents and I breathed a sigh of relief. We can't know for sure that the anti-vaccine movement, led by uninformed celebrities and discredited research, is the reason we are having such a terrible measles outbreak in the United States. But certainly it's not helping our children. Before measles vaccinations were available, every year in the United States, approximately 500,000 people were infected with measles and 500 died. That number fell to around 60 infections annually after vaccinations were introduced in 1994. The Vaccines for Children program, implemented in 1994, helped lead to the eradication of measles by the year 2000. The Centers for Disease Control and Prevention estimates that in the last 20 years, vaccinations prevented 322 million illnesses and 732,000 deaths, while saving $295 billion. Now, we can add measles to the list of illnesses that are crawling back out of the dustbin of history. Cases of measles have been on the rise since 2010, and globally, 14 people die from measles every hour. The CDC reports the majority of those who have contracted measles were not vaccinated. Some of the outbreaks have been from unvaccinated U.S. citizens getting measles abroad and bringing it back to those not vaccinated in the United States. The trend of not receiving vaccinations has been fueled by many, including Jenny McCarthy. The son of the former co-host of \"The View\" was diagnosed with autism in 2007. Since then she has been fervently critical of vaccination,\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add a padding token (GPT-2 does not have one)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Define the preprocessing function for GPT-2\n",
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "\n",
    "    # Tokenize inputs and targets together, pad to max_length\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs, text_target=targets,\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Ensure labels are padded to max_length as well\n",
    "    if \"labels\" in model_inputs:\n",
    "        labels = model_inputs[\"labels\"]\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i] + [-100] * (512 - len(labels[i])) if len(labels[i]) < 512 else labels[i][:512]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the small datasets\n",
    "train_gpt2 = small_train_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_gpt2 = small_val_dataset.map(preprocess_gpt2, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(\"Sample tokenized GPT-2 input:\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9450e92",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "#### With a Max Length to 512, Following is the result:\n",
    "- Map: 100%|██████████| 287113/287113 [03:57<00:00, 1207.51 examples/s]\n",
    "- Map: 100%|██████████| 13368/13368 [00:21<00:00, 617.03 examples/s]\n",
    "#### Max Length changed to 100, Following is the result:\n",
    "- Map: 100%|██████████| 287113/287113 [02:27<00:00, 1945.64 examples/s]\n",
    "- Map: 100%|██████████| 13368/13368 [00:08<00:00, 1618.98 examples/s]\n",
    "#### Max Length changed to 200, Following is the result:\n",
    "- Map: 100%|██████████| 287113/287113 [02:58<00:00, 1608.06 examples/s]\n",
    "- Map: 100%|██████████| 13368/13368 [00:11<00:00, 1123.31 examples/s]\n",
    "#### Max Length changed to 350, Following is the result:\n",
    "- Map: 100%|██████████| 287113/287113 [02:46<00:00, 1727.32 examples/s]\n",
    "- Map: 100%|██████████| 13368/13368 [00:08<00:00, 1647.63 examples/s]\n",
    "\n",
    "Max Length\tTrain Speed (examples/s)\tTrain Time\tVal Speed (examples/s)\tVal Time\n",
    "- 512\t    1207 /s\t                    ~3m57s\t    617 /s\t                ~21s\n",
    "- 100\t    1945 /s\t                    ~2m27s\t    1619 /s\t                ~8s\n",
    "- 200\t    1608 /s\t                    ~2m58s\t    1123 /s\t                ~11s\n",
    "- 350\t    1727 /s\t                    ~2m46s\t    1648 /s\t                ~8s\n",
    "\n",
    "### Analysis\n",
    "#### At Max Length = 512\n",
    "-\tSlowest preprocessing (only ~1207 examples/s).\n",
    "-\tLonger sequences → more tokens per example → heavier tokenization work.\n",
    "-\tEnsures full context of article/summary pairs, but at cost of speed.\n",
    "-\tUseful when articles are long and you don’t want to truncate important info.\n",
    "________________________________________\n",
    "#### At Max Length = 100\n",
    "-\tFastest preprocessing (1945 examples/s for training, 1619 for validation).\n",
    "-\tVery short sequences → quick to tokenize & pad.\n",
    "-\tRisk: losing lots of article information due to truncation → summaries may be incomplete.\n",
    "-\tLikely faster training too, but quality of generated summaries may degrade.\n",
    "________________________________________\n",
    "#### At Max Length = 200\n",
    "-\tMiddle ground. Processing speed drops compared to 100 (1608 examples/s).\n",
    "-\tStill much faster than 512, while retaining more context.\n",
    "-\tBalances efficiency and retaining content.\n",
    "________________________________________\n",
    "#### At Max Length = 350\n",
    "-\tInteresting result: faster than 200 (1727 examples/s vs 1608).\n",
    "-\tLikely due to batching and memory usage quirks: padding aligns better with GPU/CPU batch sizes → can sometimes improve speed.\n",
    "-\tValidation also ran very fast (1647 /s).\n",
    "________________________________________\n",
    "### Analysis\n",
    "-\tSpeed vs Context Trade-off:\n",
    "-\tShorter max length = faster preprocessing, but higher risk of truncating data.\n",
    "-\tLonger max length = more accurate context preserved, but slower preprocessing.\n",
    "####\tPractical Implications:\n",
    "-\tIf articles are mostly short (<100 tokens), a max length of 100 is enough → fastest and efficient.\n",
    "-\tIf articles are long (200–400 tokens), using 100 would chop critical info → bad for summarization quality.\n",
    "-\t350 seems like a sweet spot: retains substantial context while still faster than 200.\n",
    "-\t512 is safest for data quality but slowest.\n",
    "\n",
    "###\tTraining Phase Impact:\n",
    "-\tShorter max length = smaller training batches in memory = faster iterations.\n",
    "-\tLonger max length = larger memory use, slower updates, possibly fewer steps per epoch.\n",
    "________________________________________\n",
    "### Final Takeaway\n",
    "-\t100 → fastest, but risky (likely poor summaries).\n",
    "-\t200 → balance, but slower than 350.\n",
    "-\t350 → best compromise: retains more info, processes quickly (faster than 200!).\n",
    "-\t512 → maximum context preserved, but slowest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5007eb0",
   "metadata": {},
   "source": [
    "# Observation: Input max length = 512 and varied label padding length (128, 256, 384, 512). \n",
    "________________________________________\n",
    "#### Results Recap\n",
    "#### Input Max Length\tLabel Max Length\tTrain Speed (examples/s)\tTrain Time\tVal Speed (examples/s)\tVal Time\n",
    "- 512\t128\t1347/s\t~3m33s\t1367/s\t~9s\n",
    "- 512\t256\t1161/s\t~4m07s\t1220/s\t~10s\n",
    "- 512\t384\t926/s\t~5m09s\t433/s\t~30s\n",
    "- 512\t512\t1191/s\t~4m01s\t1596/s\t~8s\n",
    "________________________________________\n",
    "### Observations\n",
    "#### Labels = 128\n",
    "-\tFastest training (1347/s).\n",
    "-\tSummaries truncated/padded to 128 tokens only.\n",
    "-\tGood efficiency, but risk of information loss if many summaries exceed 128 tokens.\n",
    "________________________________________\n",
    "#### Labels = 256\n",
    "-\tSlower (1161/s).\n",
    "-\tRetains longer summaries than 128.\n",
    "-\tBalanced option, but not as efficient as 128 or 512.\n",
    "________________________________________\n",
    "#### Labels = 384\n",
    "-\tSlowest overall (926/s train, 433/s validation).\n",
    "-\tBig performance hit, especially in validation (30 seconds vs 8–10s).\n",
    "-\tLikely because padding to an “awkward” middle length increases memory use and decreases batching efficiency.\n",
    "-\tPreserves long summaries, but inefficient.\n",
    "________________________________________\n",
    "#### Labels = 512\n",
    "-\tSurprisingly faster than 256 and 384 (1191/s train, 1596/s validation).\n",
    "-\tWhy? Likely due to implementation optimization when inputs and labels match length (both 512). Vectorized better.\n",
    "-\tSafest choice for data quality (no truncation), but uses most memory during model training.\n",
    "________________________________________\n",
    "### Analysis\n",
    "####\tSpeed vs Quality Trade-off\n",
    "-\tShorter labels (128) → fastest, but summaries get cut → hurts model quality.\n",
    "-\tLonger labels (512) → more context preserved, reasonable speed → better for accuracy.\n",
    "-\t384 is the worst compromise: slower and not significantly more informative than 256 or 512.\n",
    "####\tConsistency Helps\n",
    "-\tWhen both inputs and labels are padded to 512, the pipeline runs more efficiently, even though it processes more tokens.\n",
    "####\tValidation Bottleneck at 384\n",
    "-\tValidation slowed down massively at 384 → likely due to padding mismatch across batches (less efficient batching).\n",
    "________________________________________\n",
    "### Final Takeaway\n",
    "-\t128 → Best for speed, worst for quality.\n",
    "-\t256 → Middle ground, okay if summaries are usually short.\n",
    "-\t384 → Worst choice (slow + inefficient).\n",
    "-\t512 → Best overall balance: preserves all summary info, surprisingly efficient in preprocessing, but expect heavier GPU memory use when training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67ece",
   "metadata": {},
   "source": [
    "\n",
    "### GPT‑2 Fine Tuning\n",
    "\n",
    "We use the Hugging Face `Trainer` API to fine‑tune the GPT‑2 model.  A `DataCollatorForLanguageModeling` automatically pads the inputs and labels and performs dynamic masking where appropriate (although in our custom loss masking we already set `-100` values).  The training arguments below specify a small number of epochs and batch sizes for illustration; adjust these for a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8b4cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\puttarajus\\desktop\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ec498",
   "metadata": {},
   "source": [
    "### Loss on All tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7a0222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1628.2215, 'train_samples_per_second': 0.307, 'train_steps_per_second': 0.039, 'train_loss': 3.2344224717881946, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=3.2344224717881946, metrics={'train_runtime': 1628.2215, 'train_samples_per_second': 0.307, 'train_steps_per_second': 0.039, 'train_loss': 3.2344224717881946, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "# Define data collator\n",
    "data_collator_gpt2 = DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False)\n",
    "\n",
    "# Load the GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args_gpt2 = TrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],  # disable logging to wandb\n",
    ")\n",
    "\n",
    "# Create Trainer for GPT-2\n",
    "trainer_gpt2 = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator_gpt2,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train; training can take several minutes even on small subsets\n",
    "trainer_gpt2.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a93a4",
   "metadata": {},
   "source": [
    "### Summary-only loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "792616ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:01<00:00, 361.65 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 426.54 examples/s]\n",
      "C:\\Users\\PuttarajuS\\AppData\\Local\\Temp\\ipykernel_14572\\2634961713.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_gpt2 = Seq2SeqTrainer(\n",
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 825.8914, 'train_samples_per_second': 0.605, 'train_steps_per_second': 0.076, 'train_loss': 3.0960211375403026, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=3.0960211375403026, metrics={'train_runtime': 825.8914, 'train_samples_per_second': 0.605, 'train_steps_per_second': 0.076, 'train_loss': 3.0960211375403026, 'epoch': 1.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,   # <-- use this when labels are prebuilt\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Tokenizer & model setup (pad + special SEP)\n",
    "# --------------------------------------------------\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "# GPT-2 has no pad token → reuse EOS\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Add a separator token to split article vs summary\n",
    "SEP = \"<|sep|>\"\n",
    "if SEP not in gpt2_tokenizer.get_vocab():\n",
    "    gpt2_tokenizer.add_special_tokens({\"additional_special_tokens\": [SEP]})\n",
    "\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "# IMPORTANT: resize embeddings after adding tokens\n",
    "gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Preprocessing: SUMMARY-ONLY loss w/ length budget\n",
    "#    - masks loss on prompt+article, trains only on summary\n",
    "#    - reserves space (max_new_tokens) so summary isn’t truncated\n",
    "# --------------------------------------------------\n",
    "def preprocess_gpt2_summary_only(examples, max_length=256, max_new_tokens=128):\n",
    "    prefix = \"summarize: \"\n",
    "    sep_id = gpt2_tokenizer.convert_tokens_to_ids(SEP)\n",
    "    pad_id = gpt2_tokenizer.pad_token_id\n",
    "\n",
    "    input_ids_list, attn_mask_list, labels_list = [], [], []\n",
    "\n",
    "    for art, tgt in zip(examples[\"article\"], examples[\"highlights\"]):\n",
    "        # tokenize separately so we can budget space\n",
    "        prompt_ids = gpt2_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "        art_ids    = gpt2_tokenizer.encode(art,   add_special_tokens=False)\n",
    "        tgt_ids    = gpt2_tokenizer.encode(tgt,   add_special_tokens=False)\n",
    "\n",
    "        # reserve room for SEP + summary\n",
    "        max_src = max_length - max_new_tokens - 1  # -1 for SEP\n",
    "        # trim article so prompt+article fit in max_src\n",
    "        art_ids = art_ids[: max(0, max_src - len(prompt_ids))]\n",
    "\n",
    "        # build final input\n",
    "        input_ids = prompt_ids + art_ids + [sep_id] + tgt_ids\n",
    "        input_ids = input_ids[:max_length]  # hard cap\n",
    "\n",
    "        # find SEP and mask everything up to SEP (inclusive)\n",
    "        sep_pos = input_ids.index(sep_id) if sep_id in input_ids else len(input_ids) - 1\n",
    "        labels = [-100] * (sep_pos + 1) + input_ids[sep_pos + 1:]\n",
    "\n",
    "        # pad to max_length\n",
    "        attn_mask = [1] * len(input_ids)\n",
    "        if len(input_ids) < max_length:\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [pad_id] * pad_len\n",
    "            labels    += [-100]   * pad_len\n",
    "            attn_mask += [0]      * pad_len\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attn_mask_list.append(attn_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attn_mask_list,\n",
    "        \"labels\": labels_list,\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Map your raw datasets → tokenized datasets\n",
    "#    Replace these two lines with whatever your raw sets are called.\n",
    "#    If you already have small_train_dataset/small_val_dataset, use them here.\n",
    "# --------------------------------------------------\n",
    "# Example: using `small_train_dataset` / `small_val_dataset` produced earlier\n",
    "train_gpt2 = small_train_dataset.map(\n",
    "    lambda x: preprocess_gpt2_summary_only(x, max_length=256, max_new_tokens=128),\n",
    "    batched=True,\n",
    "    remove_columns=small_train_dataset.column_names,\n",
    ")\n",
    "val_gpt2 = small_val_dataset.map(\n",
    "    lambda x: preprocess_gpt2_summary_only(x, max_length=256, max_new_tokens=128),\n",
    "    batched=True,\n",
    "    remove_columns=small_val_dataset.column_names,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Collator: simple padding (labels already set)\n",
    "#    Do NOT use DataCollatorForLanguageModeling here.\n",
    "# --------------------------------------------------\n",
    "data_collator = DataCollatorWithPadding(gpt2_tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# IMPORTANT: use the correct key: evaluation_strategy (not eval_strategy)\n",
    "training_args_gpt2 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",   # <-- fix this key\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    predict_with_generate=True,    # now valid\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# if you prebuilt labels (summary-only loss), use DataCollatorWithPadding\n",
    "# data_collator = DataCollatorWithPadding(gpt2_tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "trainer_gpt2 = Seq2SeqTrainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator,   # your pad collator\n",
    "    tokenizer=gpt2_tokenizer,\n",
    ")\n",
    "# --------------------------------------------------\n",
    "# 7) Train\n",
    "# --------------------------------------------------\n",
    "trainer_gpt2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657780f",
   "metadata": {},
   "source": [
    "### The above runs was to train GPT-2 to calcualte LOSS for all Tokens V/S LOSS for only Summary  \n",
    "### These runs tell about objective choice and context/summary budget for GPT-2 summarization.\n",
    "\n",
    "### Quick roll-up\n",
    "#### Run\tObjective\tmax_length\tmax_new_tokens\tTrain runtime\tSteps/s\tSamples/s\tTrain loss\n",
    "- R0\tAll tokens\t(assumed 512)\t—\t27m 10s\t0.039\t0.307\t3.2344\n",
    "- R1\tSummary-only\t512\t128\t46m 19s\t0.023\t0.180\t2.6723\n",
    "- R2\tSummary-only\t128\t64\t14m 25s\t0.073\t0.581\t3.4784\n",
    "- R3\tSummary-only\t128\t64\t13m 50s\t0.076\t0.605\t3.0960\n",
    "\n",
    "(Two near-identical R2/R3 runs show normal seed/shuffle variance.)\n",
    "\n",
    "Inference\n",
    "1) Objective matters (and summary-only is the right one)\n",
    "\n",
    "Summary-only loss (R1) is much lower than all-tokens loss (R0) at similar context (≈512).\n",
    "\n",
    "We're optimizing exactly what we care about—predicting the summary—instead of wasting loss on the prompt+article tokens. This usually yields less copying and better ROUGE once you evaluate with generation.\n",
    "\n",
    "Note: the numeric losses aren’t apples-to-apples (you average over different token sets), but the direction is what you want: focusing loss on summaries helps the model learn the task.\n",
    "\n",
    "2) Context (max_length) and summary budget (max_new_tokens) trade speed vs fit\n",
    "\n",
    "Shorter context (R2/R3: 128/64) is ~3.2× faster than long context (R1: 512/128), but train loss is higher (3.10–3.48 vs 2.67).\n",
    "\n",
    "With less of the article available before <|sep|>, the model has less evidence to predict the summary tokens.\n",
    "\n",
    "Bigger summary budget helps. R1 (max_new_tokens=128) beats R2/R3 (64). Longer targets let the model learn fuller summary structure instead of being forced into ultra-short endings.\n",
    "\n",
    "3) Your 512-token summary-only run is slower than the all-tokens run\n",
    "\n",
    "R1 steps/s (0.023) < R0 (0.039). That’s expected if, for R1, you used Seq2SeqTrainer with predict_with_generate=True (generation at eval steps adds overhead) and/or gradient checkpointing. The forward/backward still processes 512 tokens either way.\n",
    "\n",
    "4) Run-to-run variance at short context is real\n",
    "\n",
    "R2 vs R3 differ by ~0.38 in train loss at identical hyperparams. Lock a seed (seed=42, data_seed=42) or average 2–3 runs when reporting.\n",
    "\n",
    "#### Takeaways:\n",
    "\n",
    "Aligning the loss with the task (summary-only) gives a clear quality win and should translate to higher ROUGE/BERTScore and fewer verbatim copies.\n",
    "\n",
    "Context matters: cutting max_length from 512→128 triples throughput but hurts fit; useful when you need speed or your articles are short.\n",
    "\n",
    "Reserve target budget: larger max_new_tokens (e.g., 128 vs 64) improves learning of complete summaries.\n",
    "\n",
    "Choose a sweet spot: if your articles are often >128 tokens, try max_length=256 or 384 with max_new_tokens=128—commonly a strong speed/quality balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7cb9ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated summary: summarize: Have you considered how and when you'll withdraw from your retirement accounts? Doing so in the incorrect order may end up costing you.\n",
      "\n",
      "Do you have a plan to retire early?\n",
      "\n",
      "Do you have a plan to retire\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample input\n",
    "sample_input = \"summarize: Have you considered how and when you'll withdraw from your retirement accounts? Doing so in the incorrect order may end up costing you.\"\n",
    "inputs = gpt2_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = gpt2_model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "print(\"Generated summary:\", gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16871507",
   "metadata": {},
   "source": [
    "## What do you observe in the output?\n",
    "1. Can you postprocess the output so that it only starts printing after the input sequence?\n",
    "2. Can you iterate and improve the summary quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2485159",
   "metadata": {},
   "source": [
    "#### Postprocessing the the output so that it only starts printing after the input sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17150273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summaries:\n",
      "\n",
      "1. I've always thought it was important to ensure that I was able to afford a comfortable retirement.\n",
      "However, a number of factors have come into play.\n",
      "First, it's important to note that, while I have a retirement account in my name, I'm not entitled to any retirement benefits.\n",
      "If I have an annuity (e.g. 401(k) or 403(\n",
      "\n",
      "2. Your account will not be closed until the end of your retirement. If you choose not to withdraw from your account, you will be able to withdraw your money from your bank account at any time. If you withdraw your money from a retirement account, you may withdraw your money from your bank account only once or twice. You must also agree to pay all taxes on your withdrawal.\n",
      "What are my\n",
      "\n",
      "3. We offer you the option to withdraw your retirement account at any time during the first month of your plan's life. You can withdraw your retirement account at any time during the first month of your plan's life, at any time before your first month of your plan's life, or at any time after your first month of your plan's life. You can withdraw your retirement account at any time during the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Input text (article you want to summarize)\n",
    "input_text = \"\"\"Article: \n",
    "Have you considered how and when you'll withdraw from your retirement accounts? Doing so in the incorrect order may end up costing you.\n",
    "Summary:\"\"\"\n",
    "\n",
    "# Encode input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=80,               # generate summary tokens only\n",
    "    do_sample=True,                  # sampling instead of greedy\n",
    "    top_k=50,                        # top-k sampling\n",
    "    top_p=0.9,                       # nucleus sampling\n",
    "    temperature=0.7,                 # smoothness\n",
    "    num_return_sequences=3,          # generate multiple candidates\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Post-process: only keep the \"new\" tokens after the input\n",
    "generated_summaries = []\n",
    "for output in outputs:\n",
    "    gen_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    # Trim the input prefix → only keep the continuation\n",
    "    continuation = gen_text[len(input_text):].strip()\n",
    "    generated_summaries.append(continuation)\n",
    "\n",
    "print(\"Generated Summaries:\\n\")\n",
    "for i, summ in enumerate(generated_summaries, 1):\n",
    "    print(f\"{i}. {summ}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54fbda",
   "metadata": {},
   "source": [
    "#### Iterate and improve the summary quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbb5817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best summary: I've always thought it was important to ensure that I was able to afford a comfortable retirement.\n",
      "However, a number of factors have come into play.\n",
      "First, it's important to note that, while I have a retirement account in my name, I'm not entitled to any retirement benefits.\n",
      "If I have an annuity (e.g. 401(k) or 403(\n"
     ]
    }
   ],
   "source": [
    "# Simple heuristic: pick the shortest valid summary\n",
    "best_summary = min(generated_summaries, key=lambda x: len(x.split()))\n",
    "print(\"Best summary:\", best_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1ffa7",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation\n",
    "\n",
    "After fine‑tuning the models (training steps are commented out by default), we evaluate them on the validation subset. Different metrics are appropriate for each architecture:\n",
    "\n",
    "* **GPT‑2** (decoder‑only): We generate summaries using greedy decoding and compute ROUGE metrics (ROUGE‑1, ROUGE‑2, ROUGE‑L). We also compute perplexity using the loss returned by the trainer.\n",
    "\n",
    "* **BERT** (encoder‑only): BERT is not designed to generate full sequences; instead we use it for downstream tasks such as text classification. For a classification scenario, the evaluation metrics are typically confusion matrix and F1-score.\n",
    "\n",
    "* **T5** (encoder‑decoder): We generate summaries using greedy decoding and compute ROUGE metrics.  Perplexity is computed similarly to GPT‑2 by exponentiating the validation loss.\n",
    "\n",
    "The code below demonstrates evaluation routines for each model. Running these functions requires trained models; if you skipped training above, the evaluation will use the pre‑trained weights and therefore will not yield good summarization quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51a36d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from rouge-score) (2.3.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.1 MB/s  0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=9ebeac35905142d303169317c58992fdfb07b1d165789a22094555c12dfb022e\n",
      "  Stored in directory: c:\\users\\puttarajus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: click, absl-py, nltk, rouge-score\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [absl-py]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [rouge-score]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 click-8.2.1 nltk-3.9.1 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk rouge-score absl-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fda963",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aac13870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5808355808258057, 'eval_runtime': 40.2239, 'eval_samples_per_second': 2.486, 'eval_steps_per_second': 1.243, 'epoch': 1.0}\n",
      "GPT-2 Perplexity: 13.208\n",
      "GPT-2 ROUGE: {'rouge1': np.float64(14.32), 'rouge2': np.float64(6.07), 'rougeL': np.float64(10.11), 'rougeLsum': np.float64(12.31)}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# make sure GPT-2 can pad\n",
    "if gpt2_tokenizer.pad_token is None:\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Define ROUGE metric\n",
    "evaluate_rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics_rouge(preds, refs):\n",
    "    # Compute ROUGE scores; use newline separation between sentences in each text\n",
    "    result = evaluate_rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "# Function to generate summaries with GPT-2\n",
    "def evaluate_gpt2(model, tokenizer, dataset, num_samples=10):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for i, example in enumerate(dataset.select(range(num_samples))):\n",
    "        prompt = \"summarize: \" + example[\"article\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=512)\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        preds.append(summary)\n",
    "        refs.append(example[\"highlights\"])\n",
    "    rouge_scores = compute_metrics_rouge(preds, refs)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to compute perplexity from evaluation loss\n",
    "def compute_perplexity(eval_output):\n",
    "    loss = eval_output[\"eval_loss\"]\n",
    "    return round(torch.exp(torch.tensor(loss)).item(), 3)\n",
    "\n",
    "# Evaluate GPT-2 (if trained) -- example usage\n",
    "gpt2_eval_results = trainer_gpt2.evaluate()\n",
    "gpt2_perplexity = compute_perplexity(gpt2_eval_results)\n",
    "rouge_gpt2 = evaluate_gpt2(gpt2_model, gpt2_tokenizer, small_val_dataset)\n",
    "print(\"GPT-2 Perplexity:\", gpt2_perplexity)\n",
    "print(\"GPT-2 ROUGE:\", rouge_gpt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991da63",
   "metadata": {},
   "source": [
    "## Evaluation GPT-2 \n",
    "### GPT-2 (Decoder-only LM)\n",
    "####\tLoss / Perplexity:\n",
    "-\tEval loss: 2.58 → Perplexity ≈ 13.2 (higher = more uncertain predictions).\n",
    "####\tROUGE scores (summarization quality):\n",
    "-\tRouge-1 = 14.32\n",
    "-\tRouge-2 = 6.07\n",
    "-\tRouge-L = 10.11\n",
    "-\tRouge-Lsum = 12.31\n",
    "####\tInference:\n",
    "-\tGPT-2 can generate fluent text but struggles to capture summary content fidelity.\n",
    "-\tLow ROUGE scores suggest poor overlap with reference summaries.\n",
    "-\tSlowest runtime among the three (40s, ~2.5 samples/sec).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8a2bb",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis and discussion\n",
    "\n",
    "After fine‑tuning the models and running the evaluation routines, you should fill in a comparison of the results.  Typical observations include:\n",
    "\n",
    "- **Decoder‑only (GPT‑2):** GPT‑2 fine‑tuned on a summarization corpus learns to generate coherent summaries.  Its perplexity should decrease significantly compared with the pre‑trained model, and ROUGE scores should improve.  Because GPT‑2 has no separate encoder, it must memorize how to map the input prompt to the desired output, which can make training less sample‑efficient for conditional tasks.  However, at inference time GPT‑2 generates outputs quickly via a single decoder.\n",
    "\n",
    "- **Encoder‑only (BERT):** BERT excels at understanding tasks but struggles with generative tasks.  MLM fine‑tuning improves its perplexity on the article‑summary text, but it cannot generate full summaries.  The `fill‑mask` pipeline can fill individual tokens, but the lack of an auto‑regressive decoder makes long‑form generation impractical.  This illustrates why encoder‑only architectures are not suited for free‑form text generation.\n",
    "\n",
    "- **Encoder‑decoder (T5):** T5 is designed for text‑to‑text tasks and typically achieves the best summarization scores among the three models when fine‑tuned properly.  Its separate encoder compresses the input, and the decoder generates output conditioned on the encoded context.  T5 often yields higher ROUGE scores and lower perplexity than GPT‑2 on summarization because the architecture explicitly models conditional generation.  The trade‑off is increased computational cost due to the encoder and decoder.\n",
    "\n",
    "### Chain‑of‑thought (CoT) reasoning\n",
    "\n",
    "Chain‑of‑thought reasoning refers to models generating intermediate reasoning steps before arriving at a final answer.  Decoder‑only models (like GPT‑2 and GPT‑3) naturally support CoT prompting because they generate text token by token.  Encoder‑decoder models like T5 can also perform CoT when prompted appropriately (e.g. instructing the model to \"think step by step\").  Encoder‑only models lack a decoding mechanism and therefore are not directly applicable to CoT generation.  In practice, CoT reasoning quality improves with larger models and more sophisticated training (e.g. instruction‑tuning or reinforcement learning with human feedback), which are beyond the scope of this introductory exercise.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we implemented and compared three transformer architectures on a common summarization task using the CNN/DailyMail dataset.  We demonstrated how to fine‑tune a decoder‑only model (GPT‑2), an encoder‑only model (BERT), and an encoder‑decoder model (T5).  The code illustrated data preprocessing, training setups, and evaluation routines using ROUGE and perplexity metrics.  While only small subsets of the dataset were used for demonstration purposes, you should expand the training data and adjust hyperparameters for a thorough experiment.  The analysis underscores the strengths and limitations of each architecture and highlights why encoder‑decoder models are generally preferred for conditional text generation tasks like summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81364b3c",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformer Architecture Exercise\n",
    "Use this notebook as a starting point and expand on your understanding of transformer models by completing the following structured tasks. You are encouraged to experiment, analyze, and critically reflect on your findings in your report.\n",
    "\n",
    "## Part 1: Model Training & Implementation\n",
    "### 1. Dataset Preparation\n",
    "- Choose one standard text dataset suitable for generative tasks. Options include:\n",
    "  - CNN/DailyMail → summarization\n",
    "  - WikiText-2 → language modeling (text generation)\n",
    "  - SQuAD v1.1 → question answering\n",
    "- Briefly describe why you selected this dataset and what task you’ll evaluate (summarization, QA, or text generation).\n",
    "- Show how you preprocessed the data (tokenization, train/val split, max length, etc.).\n",
    "\n",
    "### 2. Model Implementation\n",
    "\n",
    "Implement and train the following:\n",
    "- Decoder-only model (GPT-style): e.g., GPT-2 small from Hugging Face.\n",
    "- Encoder-only model (BERT-style): e.g., BERT-base, used for masked-language-modeling or extractive QA/summarization.\n",
    "- Encoder-decoder model (T5-style): e.g., T5-small, trained for the same dataset/task as the other two.\n",
    "\n",
    "### 3. Training Documentation\n",
    "\n",
    "- Document your training setup (batch size, learning rate, optimizer, epochs, hardware).\n",
    "- Save a few training/validation loss curves or logs to show how training progressed.\n",
    "- Mention any difficulties you faced and how you addressed them (e.g., memory limits, convergence).\n",
    "\n",
    "## Part 2: Evaluation & Analysis\n",
    "\n",
    "### 4. Performance Evaluation\n",
    "\n",
    "- Evaluate all three models on the same task.\n",
    "- Report results using at least two metrics:\n",
    "  - Text generation/summarization: BLEU, ROUGE, perplexity\n",
    "  - Question answering: F1, Exact Match (EM), BLEU\n",
    "- Include 1–2 sample outputs per model to illustrate qualitative differences.\n",
    "\n",
    "### 5. Comparative Discussion\n",
    "\n",
    "- Compare the strengths and weaknesses of each architecture on your chosen task.\n",
    "- Suggested angles:\n",
    "\n",
    "  - Decoder-only: fluent text generation, but weaker at bidirectional context.\n",
    "  - Encoder-only: strong understanding of context, but not designed for open generation.\n",
    "  - Encoder-decoder: flexible, strong on conditional generation tasks (summarization, QA).\n",
    "\n",
    "- Which model seemed easiest to fine-tune?\n",
    "- Which produced the best outputs on your dataset?\n",
    "- Which was the most efficient (speed, memory)?\n",
    "\n",
    "### 6. Reflections on Applicability\n",
    "\n",
    "- In what real-world scenarios would you prefer each architecture?\n",
    "- Briefly note whether you think CoT reasoning would have helped these models if you had added it (conceptual discussion only—no experiments required)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
