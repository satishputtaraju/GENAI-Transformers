{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a61a1e",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformer Architecture Exercise\n",
    "Use this notebook as a starting point and expand on your understanding of transformer models by completing the following structured tasks. You are encouraged to experiment, analyze, and critically reflect on your findings in your report.\n",
    "\n",
    "## Part 1: Model Training & Implementation\n",
    "### 1. Dataset Preparation\n",
    "- Choose one standard text dataset suitable for generative tasks. Options include:\n",
    "  - CNN/DailyMail → summarization\n",
    "  - WikiText-2 → language modeling (text generation)\n",
    "  - SQuAD v1.1 → question answering\n",
    "- Briefly describe why you selected this dataset and what task you’ll evaluate (summarization, QA, or text generation).\n",
    "- Show how you preprocessed the data (tokenization, train/val split, max length, etc.).\n",
    "\n",
    "### 2. Model Implementation\n",
    "\n",
    "Implement and train the following:\n",
    "- Decoder-only model (GPT-style): e.g., GPT-2 small from Hugging Face.\n",
    "- Encoder-only model (BERT-style): e.g., BERT-base, used for masked-language-modeling or extractive QA/summarization.\n",
    "- Encoder-decoder model (T5-style): e.g., T5-small, trained for the same dataset/task as the other two.\n",
    "\n",
    "### 3. Training Documentation\n",
    "\n",
    "- Document your training setup (batch size, learning rate, optimizer, epochs, hardware).\n",
    "- Save a few training/validation loss curves or logs to show how training progressed.\n",
    "- Mention any difficulties you faced and how you addressed them (e.g., memory limits, convergence).\n",
    "\n",
    "## Part 2: Evaluation & Analysis\n",
    "\n",
    "### 4. Performance Evaluation\n",
    "\n",
    "- Evaluate all three models on the same task.\n",
    "- Report results using at least two metrics:\n",
    "  - Text generation/summarization: BLEU, ROUGE, perplexity\n",
    "  - Question answering: F1, Exact Match (EM), BLEU\n",
    "- Include 1–2 sample outputs per model to illustrate qualitative differences.\n",
    "\n",
    "### 5. Comparative Discussion\n",
    "\n",
    "- Compare the strengths and weaknesses of each architecture on your chosen task.\n",
    "- Suggested angles:\n",
    "\n",
    "  - Decoder-only: fluent text generation, but weaker at bidirectional context.\n",
    "  - Encoder-only: strong understanding of context, but not designed for open generation.\n",
    "  - Encoder-decoder: flexible, strong on conditional generation tasks (summarization, QA).\n",
    "\n",
    "- Which model seemed easiest to fine-tune?\n",
    "- Which produced the best outputs on your dataset?\n",
    "- Which was the most efficient (speed, memory)?\n",
    "\n",
    "### 6. Reflections on Applicability\n",
    "\n",
    "- In what real-world scenarios would you prefer each architecture?\n",
    "- Briefly note whether you think CoT reasoning would have helped these models if you had added it (conceptual discussion only—no experiments required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd2458",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 2: Transformer Architecture Exercise\n",
    "\n",
    "This notebook serves as a reference implementation for **Assignment 2** of the generative AI course.  The goal is to compare three prominent transformer architectures—**decoder‑only**, **encoder‑only**, and **encoder‑decoder**—on a common generative task.  The assignment requires training each architecture on the same dataset, evaluating their performance with common metrics, and analysing the implications of architectural differences on generative tasks and chain‑of‑thought reasoning.\n",
    "\n",
    "## Dataset selection\n",
    "\n",
    "For this exercise we use the **CNN/DailyMail** summarisation dataset (version `3.0.0`) from Hugging Face’s `datasets` library.  The dataset comprises news articles paired with human‑written summaries; each article–summary pair provides a natural input/output example for a generative model.  Because the data are already split into training/validation/test splits and are widely used for abstractive summarisation research, this dataset is appropriate for comparing generative architectures.  Although `WikiText` could be used for language modelling tasks, summarisation requires models to generate structured output given an input, which better illustrates differences between decoder‑only, encoder‑only, and encoder‑decoder designs.  For compute efficiency in this notebook we subsample the dataset (e.g. a few hundred training examples) rather than using the full corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aece2b5",
   "metadata": {},
   "source": [
    "\n",
    "## Overview of transformer architectures\n",
    "\n",
    "We train three different transformer models:\n",
    "\n",
    "* **Decoder‑only (GPT‑style):** These models consist of stacked self‑attention blocks in which each token can attend only to previous tokens (causal masking).  We use `GPT‑2` as the base model and fine‑tune it to generate a summary from an article.  Because GPT‑2 is a pure language model, we construct input prompts of the form `\"summarize: <article>\"` and train the model to predict the target summary.  During training we mask out the prompt part of the input so that the loss is computed only on the summary tokens.\n",
    "\n",
    "* **Encoder‑only (BERT‑style):** Encoder‑only models such as `BERT` learn bi‑directional contextual representations using masked language modelling (MLM).  They are not inherently generative; they excel at understanding tasks (e.g. classification, token classification).  For a fair comparison on generative tasks we fine‑tune BERT on the same corpus using MLM, combining article and summary text into a single sequence.  At evaluation time we assess perplexity and use the `fill‑mask` capability to approximate generation.  This highlights BERT’s limitations on tasks requiring free‑form generation.\n",
    "\n",
    "* **Encoder‑decoder (T5‑style):** Models like `T5` encode the input sequence with an encoder and decode the output sequence with a separate decoder.  They can perform a wide range of text‑to‑text tasks, including summarisation and question answering.  We fine‑tune `T5‑small` on the CNN/DailyMail dataset using the standard prefix `\"summarize: \"` in the input to indicate the task.  During evaluation we compute ROUGE metrics on generated summaries.\n",
    "\n",
    "The following sections implement data loading, preprocessing, model fine‑tuning, and evaluation for each architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd3c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3.12.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "numpy       : 2.3.3\n",
      "matplotlib  : 3.10.6\n",
      "scikit-learn: 1.7.2\n",
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Using Python {sys.version.split()[0]}\")\n",
    "\n",
    "# Install required packages into the current notebook environment\n",
    "%pip install -qU numpy matplotlib scikit-learn\n",
    "\n",
    "# Verify versions\n",
    "import numpy as np, matplotlib, sklearn\n",
    "print(\"numpy       :\", np.__version__)\n",
    "print(\"matplotlib  :\", matplotlib.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf7bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: torch in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets transformers evaluate\n",
    "!pip install torch\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307068b7",
   "metadata": {},
   "source": [
    "## Load and inspect the dataset\n",
    "\n",
    "We load the CNN/DailyMail dataset using the Hugging Face `datasets` library.  To accelerate training for demonstration purposes we take a small subset of the training and validation sets (e.g. 500 training examples and 100 validation examples).  Each record contains two fields:\n",
    "\n",
    "* `\"article\"`: the news article text (input).\n",
    "* `\"highlights\"`: the human‑written summary (target).\n",
    "\n",
    "Below we load the dataset, inspect a few examples, and create the smaller subsets used for fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02731d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "Example training record: {'article': '(CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So far this year, measles has infected 593 people in 21 states. As a physician who treats children with compromised immune systems and as a mother of 2-year-old twins -- too young to be fully vaccinated, I am deeply concerned. A few months ago, a 4-year-old girl came to my clinic with frequent cold symptoms and infections. One way to see if her immune system functioned properly was to check her response to childhood vaccines. But because the girl\\'s mother had decided against vaccinating her, I could not perform the vaccine blood test. My hands were tied behind my back; I had no way of knowing if there was a deficiency in her immune system. And if she did have a deficiency, she had no protection from the potentially deadly diseases the vaccines are designed to inoculate. As it turned out, she didn\\'t have a life-threatening infection -- this time. Her parents and I breathed a sigh of relief. We can\\'t know for sure that the anti-vaccine movement, led by uninformed celebrities and discredited research, is the reason we are having such a terrible measles outbreak in the United States. But certainly it\\'s not helping our children. Before measles vaccinations were available, every year in the United States, approximately 500,000 people were infected with measles and 500 died. That number fell to around 60 infections annually after vaccinations were introduced in 1994. The Vaccines for Children program, implemented in 1994, helped lead to the eradication of measles by the year 2000. The Centers for Disease Control and Prevention estimates that in the last 20 years, vaccinations prevented 322 million illnesses and 732,000 deaths, while saving $295 billion. Now, we can add measles to the list of illnesses that are crawling back out of the dustbin of history. Cases of measles have been on the rise since 2010, and globally, 14 people die from measles every hour. The CDC reports the majority of those who have contracted measles were not vaccinated. Some of the outbreaks have been from unvaccinated U.S. citizens getting measles abroad and bringing it back to those not vaccinated in the United States. The trend of not receiving vaccinations has been fueled by many, including Jenny McCarthy. The son of the former co-host of \"The View\" was diagnosed with autism in 2007. Since then she has been fervently critical of vaccination, claiming vaccines such as those for measles, mumps and rubella cause autism. She founded an organization for the cause, wrote books on the topic and made countless appearances -- all the while arguing, absent any scientific data, that vaccines cause autism. McCarthy\\'s views have been influential in fanning parents\\' fears of vaccinating their children. She backtracked her stance on vaccines in an op-ed she wrote in April 2014, claiming she is not \"anti-vaccine,\" but the damage has been done. Another harmful influence was former British surgeon and researcher Andrew Wakefield. In 1998, he presented a highly publicized Lancet report connecting autism and vaccines. Later, the study was discredited by many sources. In 2010, the British General Medical Council declared his paper linking vaccine and autism was not only based on bad science, but also a deliberate fraud. For his unethical behavior he lost his medical license. But his fraudulent work has contributed to a decrease in vaccinations and increase in measles. The only credible association between autism and vaccine is age. When a child is diagnosed with a developmental delay such as autism, parents naturally search for a cause and look to recent events in the child\\'s life. Since the age of onset of autism is similar to when children receive vaccinations, some parents falsely connect these two unrelated events. Autism, in fact, appears to be rising among unvaccinated children as well. While parents have a right to be skeptical and well-informed when it comes to their children\\'s health, they must not make medical decisions based on unfounded diatribes from a celebrity or fraudulent medical paper. 5 things you don\\'t know about measles . Measles outbreaks in our own backyard should prompt educated discussion with patient families who are not vaccinated. We have many credible studies demonstrating that the advantages of vaccination outweigh the disadvantages. Vaccines reduce the threat of disease or death by a particular pathogen. Unvaccinated children face challenges when it comes to getting lifesaving care. Every 911 call, doctor appointment, or emergency room visit, you must alert medical personnel of your child\\'s vaccination status so he receives distinctive treatment. Because unvaccinated children can require treatment that is out of the ordinary, medical staff may be less experienced with the procedures required to appropriately treat your unvaccinated child. As was the case with the 4-year-old patient I saw, doctors are not be able to adequately assess a patient\\'s health if the patient is not vaccinated. During pregnancy, women who are not vaccinated can be vulnerable to diseases that may complicate their pregnancy. If an unvaccinated pregnant woman contracts rubella, her baby may have congenital rubella syndrome, causing heart defects, developmental delays and deafness. Choosing to not vaccinate your child not only puts your own child at risk, but it also puts others at risk if their child isn\\'t protected. Some people cannot be vaccinated because of other medical conditions, such as cancer. These people rely on the general public being vaccinated so their risk of exposure is reduced. According to CDC statistics comparing pre-vaccine era deaths to now, there has been a 99% decrease in measles. I know I will continue to give my own children vaccinations to protect their life and health. I urge everyone to do the same for the sake of all children.', 'highlights': 'The U.S. is experiencing the worst measles outbreak in 20 years .\\nJennifer Shih: The anti-vaccine movement can be blamed for scaring parents .\\nShe says widespread vaccination was responsible for eradicating measles years ago .\\nShih: Choosing to not vaccinate your child is bad for your child and other children .', 'id': 'a592df3cff8b44b5bb7222f638a0e6de402f9534'}\n",
      "Example validation record: {'article': '(CNN)\"A picture of horror.\" That\\'s how German Foreign Minister Frank-Walter Steinmeier described the site where a Germanwings Airbus A320 plane crashed in the French Alps on Tuesday. \"The grief of the families and loved ones is immeasurable,\" Steinmeier said, after flying over the area in the Alps in southeastern France. \"We must stand with them. We are all united in great grief.\" Departure: Barcelona, Spain, at 10:01 a.m. (26 minutes late) Destination: Scheduled to land in Dusseldorf, Germany, at 11:39 a.m. Passengers: 150 (144 passengers, six crew members) Airplane: Airbus A320 (twin-jet) Airline: Germanwings (budget airline owned by Lufthansa) Flight distance: 726 miles . Last known tracking data: 10:38 a.m. Last known speed: 480 mph . Last known altitude: 11,400 feet . Last known location: Near Digne-les-Bains, France, in the Alps . Sources: CNN and flightaware.com . Flight 9525 took off just after 10 a.m. Tuesday from Barcelona, Spain, for Dusseldorf, Germany, with 144 passengers -- among them two babies -- and six crew members. It went down at 10:53 a.m. (5:53 a.m. ET) in a remote area near Digne-les-Bains in the Alpes de Haute Provence region. All aboard are presumed dead. Helicopter crews found the airliner in pieces, none of them bigger than a small car, and human remains strewn for several hundred meters, according to Gilbert Sauvan, a high-level official in the Alpes de Haute Provence region who is being briefed on the operation. Authorities were not able to retrieve any bodies Tuesday, with the frozen ground complicating the effort. Wednesday may not be much easier, with snow in the forecast. Spanish and German officials moved to join hundreds of French firefighters and police in the area, working together to help in the recovery effort and try to figure out exactly what happened. As of Tuesday evening, there were few clues. One of the aircraft\\'s data recorders, the so-called black boxes, has been found, according to French Interior Minister Bernard Cazeneuve, but it was too early to tell what it would say about the crash. \"We don\\'t know much about the flight and the crash yet,\" German Chancellor Angela Merkel said. \"And we don\\'t know the cause.\" Relatives of those believed to be on the flight, fearing the worst, gathered at the Barcelona airport, where a crisis center was set up. French authorities set up a chapel near the crash site. Lufthansa Group said the company will look after the relatives of those on board. \"There will be a contact center established in France; relatives who would like to take advantage of this will be transferred to the contact center at no cost -- and their accommodation paid for -- just as soon as the center has been established,\" Lufthansa said. Those aboard included a \"high number of Spaniards, Germans and Turks,\" according to Spain\\'s King Felipe VI. Germanwings CEO Thomas Winkelmann said it\\'s believed 67 people, or nearly half those on the plane, are German citizens. Germanwings crash: Who was on the plane? Sixteen students and two teachers from one German high school, called Joseph Koenig Gymnasium, were among those booked on Flight 9525, according to Florian Adamik, a municipal official in Haltern, the town where the school is located. A crisis center has been established at the city hall in Haltern, which is about 77 kilometers (48 miles) north of Dusseldorf\\'s airport. Winkelmann confirmed the 16 students and two teachers were on the plane. Haltern\\'s mayor, Bodo Klimpel, said they had been heading home after taking part in a foreign exchange program. \"The whole city is shocked, and we can feel it everywhere,\" Klimpel said. A Dutch citizen and a Belgian -- the latter a resident of Barcelona -- were among those on the flight, according to those countries\\' foreign ministries. Two Australians and two Colombians were also believed to be on board. Germanwings started in 2002 and was taken over by Lufthansa seven years later as its low-cost airline, handling an increasing number of midrange flights around Europe. It was forced to cancel some flights Tuesday because there were crews that didn\\'t want to fly upon hearing news of the crash. The valley where the plane went down is long and snow-covered, and access is difficult, said the mayor of the nearby town of Barcelonnette, Pierre Martin-Charpenel. It was well populated in the 19th century but there are almost no people living there now, he said. It\\'s an out-of-the-way place with magnificent scenery, he said. The sports hall of a local school has been freed up to take in bodies of the victims of the plane crash, said Sandrine Julien from the town hall of Seyne-les-Alpes village. Seyne-les-Alpes is about 10 kilometers from the crash site. Mountain guide Yvan Theaudin told BFMTV the crash was in the area of the Massif des Trois Eveches, where there are peaks of nearly 3,000 meters (1.9 miles). It\\'s very snowy in the area and the weather is worsening, he said, which could complicate search and rescue efforts. Responders may have to use skis to reach the crash site on the ground, he said. Sandrine Boisse, president of the tourism office at the Pra Loup ski resort, said she heard the plane crash and called the police and the local government office to find out what had happened. \"It was about 11 (a.m.) here. I was outside the garage, and we heard a strange noise, and at first we thought it was an avalanche,\" she said. \"Something was wrong. ... We didn\\'t know what.\" A mountain guide who heard a plane fly at alarmingly low altitude shortly before the crash, Michel Suhubiette, said helicopters may be the only way to get to the crash site. According to the U.S. Federal Aviation Administration, just under 16% of aviation accidents occur during the cruise portion of a flight -- meaning after the climb and before descent. Accidents are more common during takeoff and landing. The twin-engine Airbus A320s, which entered service in 1988, is generally considered among the most reliable aircraft, aviation analyst David Soucie said. The captain of the crashed plane had flown for Germanwings for more than 10 years, and had more than 6,000 flight hours on this model of Airbus. The plane itself dates to 1991 and was last checked in Dusseldorf on Monday, according to Winkelmann. So what happened? CNN aviation analyst Mary Schiavo said the plane\\'s speed is one clue. According to Germanwings, the plane reached its cruising altitude of 38,000 feet, and then dropped for eight minutes. The plane lost contact with French radar at a height of approximately 6,000 feet. Then it crashed. This could indicate that there was not a stall, but that the pilot was still controlling the plane to some extent, Schiavo said. Had there been an engine stall, the plane would have crashed in a matter of minutes, she said. That small piece of information about the descent means that the pilot could have been trying to make an emergency landing, or that the plane was gliding with the pilot\\'s guidance, Schiavo said. A scenario where the plane was gliding is potentially more dangerous because wide fields for landing would be hard to come by in the mountains, she said. The crash spurred officials in several countries to offer their condolences and pledge solidarity and cooperation to help those affected and determine what happened. \"Our thoughts and our prayers are with our friends in Europe, especially the people of Germany and Spain, following the terrible airplane crash in France,\" U.S. President Barack Obama told reporters. \"It\\'s particularly heartbreaking because it apparently includes the loss of so many children, some of them infants.\" Germany\\'s Merkel said she was sending two ministers to France on Tuesday and would travel to the crash site on Wednesday to see it for herself. \"We have to think of the victims and their families and their friends,\" she said. German Foreign Minister Frank-Walter Steinmeier said the German government had set up a crisis center in response to the \"terrible news\" and was in close contact with the French authorities. \"In these difficult hours, our thoughts are with those who have to fear that their close ones are among the passengers and crew,\" he said. CNN\\'s Mariano Castillo, Hala Gorani, Laura Akhoun, Stephanie Halasz, Lindsay Isaac, Josh Levs, Richard Greene, Karl Penhaul and Sara Delgrossi contributed to this report.', 'highlights': 'The plane reached 38,000 feet, and then dropped for eight minutes, Germanwings says .\\nVictims from Germany, France, Spain, Turkey, Belgium, Holland, Colombia, Australia .\\nOne data recorder found from Germanwings plane that crashed in Alps .', 'id': '95ecfc3b12bb6bd0bee29ea2b52519634ac4279e'}\n"
     ]
    }
   ],
   "source": [
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=50).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=50).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(\"Example training record:\", small_train_dataset[0])\n",
    "print(\"Example validation record:\", small_val_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3de767",
   "metadata": {},
   "source": [
    "\n",
    "## Encoder‑only model: BERT fine‑tuning\n",
    "\n",
    "BERT uses bi‑directional self‑attention and is optimised for understanding rather than generation.  To apply BERT on our corpus we fine‑tune it using the **masked language modelling (MLM)** objective.  We concatenate the article and its summary into a single sequence and randomly mask tokens using `DataCollatorForLanguageModeling`.  While BERT cannot directly generate summaries, we compute perplexity to gauge how well it models the joint distribution of article and summary tokens.  At evaluation we also demonstrate how to use the `fill-mask` pipeline to generate single masked words as an illustration of BERT’s generative limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54798915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 samples of the dataset:\n",
      "small_train_dataset[0]: dict_keys(['article', 'highlights', 'id'])\n",
      "\t\tsmall_train_dataset[0]: (CNN) -- The worst measles outbreak in 20 years continues to grow. Most recently, there was a new case in Ohio, where there are already 377 cases. So ... \n",
      "\t->\n",
      "\t\tThe U.S. is experiencing the worst measles outbreak in 20 years .\n",
      "Jennifer Shih: The anti-vaccine movement can be blamed for scaring parents .\n",
      "She says widespread vaccination was responsible for eradicating measles years ago .\n",
      "Shih: Choosing to not vaccinate your child is bad for your child and other children .\n",
      "\t\tsmall_train_dataset[1]: By . Mark Duell for MailOnline . Speech: Joey Boyes (right) wrote the song for the wedding of his best friend Dominic Clark (left) The Disney film fro... \n",
      "\t->\n",
      "\t\tJoey Boyes wrote version for his best friend Dominic Clark's wedding .\n",
      "'Best Man Joe' based on Disney song and took him six months to plan .\n",
      "Mr Clark married Rebecca last Saturday and couple now live in Leeds .\n",
      "\t\tsmall_train_dataset[2]: The Selfies at Funerals page on Tumblr is a collection of, you guessed it, cell phone self-portraits snapped before, during and post-memorial service ... \n",
      "\t->\n",
      "\t\tSelfies at Funerals documents self-portraits people take at memorial services .\n",
      "The tasteless pictures are making the social media rounds .\n",
      "Lizzie Post of the Emily Post Institute says selfies at funerals are never appropriate .\n",
      "Post says dress conservatively, prepare a kind word and offer to help out at a memorial .\n",
      "\t\tsmall_train_dataset[3]: By . Leon Watson . PUBLISHED: . 08:50 EST, 5 January 2013 . | . UPDATED: . 10:59 EST, 5 January 2013 . A millionaire's 'perfect' family holiday ended ... \n",
      "\t->\n",
      "\t\tDavid Foster, 76, was admitted to Annecy Hospital .\n",
      "He was flown back to England eleven days later .\n",
      "Mr Foster died from  massive internal bleeding .\n",
      "Was first time he'd been away with his grandchildren .\n",
      "\t\tsmall_train_dataset[4]: Bayern Munich remain top of the Bundesliga after thrashing Hannover 96 at the Allianz Arena on Saturday. Robert Lewandowski's double either side of Ar... \n",
      "\t->\n",
      "\t\tBayern remain top of the Bundesliga after a comfortable 4-0 win over Hannover 96 at the Allianz Arena .\n",
      "A first-half double from Robert Lewandowski and a Arjen Robben goal put the hosts firmly in the ascendancy .\n",
      "Robben added a second after the break as Bayern enjoyed a convincing  win at the Allianz Arena .\n",
      "\t\tsmall_train_dataset[5]: By . Associated Press Reporter . She once called Donald Trump 'a maggot, a cockroach and a crumb.' This week, he remembered her as 'an impossible pers... \n",
      "\t->\n",
      "\t\tVera Coking, 91, is sells Atlantic City boarding house at auction for $530,000 .\n",
      "Coking has been fighting off developers since the 1970s .\n",
      "Trump tried to buy her property to use for his Trump Plaza Hotel and Casino .\n",
      "She called Trump 'a maggot, a cockroach and a crumb'\n",
      "It has been listed for sale for $995,000 since September but the auction starts at $199,000 .\n",
      "\t\tsmall_train_dataset[6]: ISTANBUL, Turkey -- Nearly two months ago, President Obama embarked on a two-day, two-city charm offensive in Turkey, a predominantly Muslim country a... \n",
      "\t->\n",
      "\t\tPoll says only 9 percent of Turks had positive view of U.S. in 2007 .\n",
      "Iraq war, foreign policy of Bush administration among the reasons, Ivan Watson says .\n",
      "President Obama tried to change those sentiments during April trip to Turkey .\n",
      "Some Turks say Obama's won them over; others wait and see, Watson reports .\n",
      "\t\tsmall_train_dataset[7]: By . Associated Press . PUBLISHED: . 17:06 EST, 4 November 2013 . | . UPDATED: . 17:44 EST, 4 November 2013 . A 23-year-old man was convicted of . sec... \n",
      "\t->\n",
      "\t\tWashington Redskins safety Sean Taylor was murdered in November 2007  during a burglary .\n",
      "Jury deliberated for 16 hours .\n",
      "Rivera allegedly confessed to killing Taylor while trying to break into Taylor's home to steal $200,000 in cash .\n",
      "Attorney said his confession was given to police under duress .\n",
      "Because Rivera was only 17 at the time, the maximum sentence is life in prison and not the death penalty .\n",
      "\t\tsmall_train_dataset[8]: Grandparents who took over raising their grandchild when she was aged four after discovering she had been subjected to horrific abuse have shared thei... \n",
      "\t->\n",
      "\t\tEvelyn and Ralph took in their granddaughter, Melissa, when she was four .\n",
      "She was born into a violent relationship .\n",
      "Their daughter was a victim of domestic abuse but refused to leave partner .\n",
      "Grandparents didn't realise quite how bad situation was till child moved in .\n",
      "She could barely speak and was often frightened .\n",
      "They discovered she had been neglected and witness self-harm .\n",
      "She had even been victim of sexual abuse .\n",
      "They sought help of NSPCC to help Melissa overcome her ordeal .\n",
      "Now age nine, she's finally beginning to have a happy childhood .\n",
      "\t\tsmall_train_dataset[9]: (CNN) -- With Eric Shinseki's resignation Friday, the top post at the Department of Veterans Affairs goes to interim Secretary Sloan Gibson. Gibson wa... \n",
      "\t->\n",
      "\t\tPresident Obama lays out some priorities for the next VA leader .\n",
      "The VA's interim secretary will be Sloan Gibson, ex-USO leader .\n",
      "Obama: Priority is for all veterans who want medical appointments to get them in timely manner .\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 samples of the dataset\n",
    "print(\"First 10 samples of the dataset:\")\n",
    "print(\"small_train_dataset[0]:\", small_train_dataset[0].keys())\n",
    "for i in range(10):\n",
    "    print(f\"\\t\\tsmall_train_dataset[{i}]: {small_train_dataset[i]['article'][:150]}... \\n\\t->\\n\\t\\t{small_train_dataset[i]['highlights']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e36fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2657.32 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2386.61 examples/s]\n",
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2199, 'grad_norm': 20.35354232788086, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 1.9790267944335938, 'eval_runtime': 20.2008, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 1.238, 'epoch': 0.8}\n",
      "{'train_runtime': 278.4993, 'train_samples_per_second': 1.795, 'train_steps_per_second': 0.449, 'train_loss': 2.2291293334960938, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-mlm\\\\tokenizer_config.json',\n",
       " './bert-mlm\\\\special_tokens_map.json',\n",
       " './bert-mlm\\\\vocab.txt',\n",
       " './bert-mlm\\\\added_tokens.json',\n",
       " './bert-mlm\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model for BERT\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define preprocessing: combine article and summary\n",
    "\n",
    "def preprocess_bert(examples):\n",
    "    concatenated_texts = [\n",
    "        art + \" \" + summ \n",
    "        for art, summ in zip(examples[\"article\"], examples[\"highlights\"])\n",
    "    ]\n",
    "    model_inputs = bert_tokenizer(concatenated_texts, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_bert = small_train_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_bert = small_val_dataset.map(preprocess_bert, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "# Data collator with MLM\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# Training arguments for BERT\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    ")\n",
    "\n",
    "# Uncomment the line below to train the BERT model\n",
    "trainer_bert.train()\n",
    "# save the model\n",
    "bert_model.save_pretrained(\"./bert-mlm\")\n",
    "bert_tokenizer.save_pretrained(\"./bert-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa895d8",
   "metadata": {},
   "source": [
    "## BERT (MLM) fine-tuning runs with only max_length changing.\n",
    "### Consolidated results\n",
    "#### max_len\tTrain runtime\tsteps/s\tsamples/s\ttrain_loss\teval_loss\tgrad_norm\n",
    "- 512\t27m 23s\t0.076\t0.305\t2.167\t1.838\t8.11\n",
    "- 384\t12m 05s\t0.173\t0.693\t2.138\t1.897\t8.65\n",
    "- 256\t8m 08s\t0.257\t1.029\t2.173\t1.881\t10.10\n",
    "- 128\t4m 40s\t0.449\t1.795\t2.229\t1.979\t20.35\n",
    "\n",
    "CPU training; pin_memory warning confirms no GPU.\n",
    "\n",
    "1.\tSpeed scales strongly with shorter sequences.\n",
    "-\tvs 512 tokens, throughput improves by ×2.3 (384), ×3.4 (256), ×5.9 (128) (steps/s). Token length dominates compute in transformers.\n",
    "2.\tValidation loss favors more context.\n",
    "-\tBest eval_loss at 512 = 1.838.\n",
    "-\t256 is only ~2.3% worse than 512 (1.881 vs 1.838) while being ~3.4× faster → excellent cost/quality trade-off.\n",
    "-\t128 degrades more clearly (≈ +7.6% worse than 512).\n",
    "3.\tShort contexts are noisier/less stable.\n",
    "-\tgrad_norm rises as length drops (512→128: +151%), a sign of noisier updates (fewer masked tokens per batch) and higher risk of instability/overfit.\n",
    "-\t384 has the lowest train_loss but not the best eval_loss → mild overfit/variance; average across seeds to confirm.\n",
    "\n",
    "### Practical takeaways\n",
    "- Recommended default: max_length=256 for speed+quality balance on CPU. Use 384/512 only if your texts often exceed 256 tokens or you need the very best MLM fit.\n",
    "- Keep “masked tokens per update” roughly constant across lengths to make comparisons fair and tame grad spikes: masked tokens/step≈batch×max_length×mlm_prob\\text{masked tokens/step} \\approx \\text{batch} \\times \\text{max\\_length} \\times \\text{mlm\\_prob}masked tokens/step≈batch×max_length×mlm_prob. If you halve max_length, double effective batch (via per_device_batch_size and/or gradient_accumulation_steps).\n",
    "- Clamp gradient growth at short lengths: set max_grad_norm=1.0 in TrainingArguments. Also consider slightly higher weight decay (e.g., 0.02) for 128/256 runs.\n",
    "- Use dynamic padding (e.g., DataCollatorForLanguageModeling(..., pad_to_multiple_of=8)) to avoid wasted compute when sequences vary.\n",
    "- Reduce CPU warnings & speed up I/O: set dataloader_pin_memory=False (CPU), and dataloader_num_workers=2–4.\n",
    "- Report a quality metric users recognize: for MLM, also show pseudo-perplexity = exp(eval_loss) and (more important) verify gains on a downstream task (e.g., your classification or NER) — domain-adaptive MLM shines when it improves downstream accuracy/F1.\n",
    "- Replicate: run each setting with 2–3 seeds; the 384 vs 256 eval gap is small enough that variance can flip them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe03250",
   "metadata": {},
   "source": [
    "### Below is the program for Whole Word Masking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160d5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1949.53 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2405.39 examples/s]\n",
      "C:\\Users\\PuttarajuS\\AppData\\Local\\Temp\\ipykernel_23704\\1493923876.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_bert = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7122, 'grad_norm': 10.66284465789795, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.260239601135254, 'eval_runtime': 39.4354, 'eval_samples_per_second': 2.536, 'eval_steps_per_second': 0.634, 'epoch': 0.8}\n",
      "{'train_runtime': 544.5144, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.23, 'train_loss': 2.648376007080078, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-mlm\\\\tokenizer_config.json',\n",
       " './bert-mlm\\\\special_tokens_map.json',\n",
       " './bert-mlm\\\\vocab.txt',\n",
       " './bert-mlm\\\\added_tokens.json',\n",
       " './bert-mlm\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForWholeWordMask,    # <-- needs recent transformers\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 1) Tokenizer & model\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# 2) Preprocessing\n",
    "def preprocess_bert(examples, max_length=256):\n",
    "    concatenated_texts = [a + \" \" + s for a, s in zip(examples[\"article\"], examples[\"highlights\"])]\n",
    "    return bert_tokenizer(\n",
    "        concatenated_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        # dynamic padding is fine; collator can pad to multiple of 8\n",
    "        # padding=False\n",
    "    )\n",
    "\n",
    "# NOTE: use column names from the dataset you’re mapping over\n",
    "train_bert = small_train_dataset.map(\n",
    "    lambda x: preprocess_bert(x, max_length=256),\n",
    "    batched=True,\n",
    "    remove_columns=small_train_dataset.column_names,\n",
    ")\n",
    "val_bert = small_val_dataset.map(\n",
    "    lambda x: preprocess_bert(x, max_length=256),\n",
    "    batched=True,\n",
    "    remove_columns=small_val_dataset.column_names,\n",
    ")\n",
    "\n",
    "# 3) Collator (WWM). If this import fails, see fallback below.\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForWholeWordMask(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=mlm_probability,\n",
    "    pad_to_multiple_of=8,      # nicer tensor shapes\n",
    ")\n",
    "\n",
    "# 4) Training args (fix key!)\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",   # <-- fixed\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),  # avoid CPU warning\n",
    "    dataloader_num_workers=2,\n",
    "    max_grad_norm=1.0,  # helpful stability at short max_length\n",
    "    seed=42, data_seed=42,\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    "    tokenizer=bert_tokenizer,   # optional but useful for saves/logs\n",
    ")\n",
    "\n",
    "trainer_bert.train()\n",
    "\n",
    "# 5) Save\n",
    "bert_model.save_pretrained(\"./bert-mlm\")\n",
    "bert_tokenizer.save_pretrained(\"./bert-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09609208",
   "metadata": {},
   "source": [
    "# Obervation from Whole Word Masking: \n",
    "### Whole-Word Masking (WWM) results say when you vary max_length:\n",
    "### Quick roll-up\n",
    "#### Setting\tTrain runtime\tSteps/s\tTrain loss\tEval loss\tPseudo-PPL exp(eval_loss)\tGrad norm\n",
    "-    WWM, max_len=128\t4m 43.7s\t0.443\t2.7063\t2.5919\t13.36\t16.63\n",
    "-    WWM, max_len=256\t9m 6.7s\t0.230\t2.6484\t2.2602\t9.59\t10.66\n",
    "\n",
    "Speed: 128 is ~1.9× faster than 256 (steps/s 0.443 → 0.230).\n",
    "- Fit/quality: 256 has ~12.8% lower eval loss (2.26 vs 2.59) → markedly better MLM fit.\n",
    "- Stability: Grad norm is much higher at 128 (16.6 vs 10.7) → noisier updates at short context.\n",
    "\n",
    "#### Inference\n",
    "1.\tMore context helps WWM learn better.\n",
    "- WWM masks entire words; with max_len=256, the model sees more surrounding context for each masked word, so predicting them is easier → lower eval loss / pseudo-perplexity.\n",
    "2.\tShorter sequences trade quality for speed.\n",
    "- 128 tokens gives you ~2× throughput, but with a worse fit (eval loss ↑ and pseudo-PPL ↑ from ~9.6 → ~13.4). If your texts are short, that hit might be acceptable; otherwise, 256 is a better default.\n",
    "3.\tHigher grad norms at 128 signal instability.\n",
    "- Shorter inputs = fewer masked words per update → noisier gradients. That’s why grad_norm spikes for 128. You can tame this (see tips below).\n",
    "4.\tWWM losses will be higher than subword-MLM losses.\n",
    "- That’s expected: masking whole words is a harder objective than masking random subword pieces. Judge WWM runs against other WWM runs, not against subword-MLM numbers.\n",
    "\n",
    "### Practical guidance\n",
    "-\tDefault pick: Use max_len=256 for WWM on news-like text: strong quality, still reasonable speed. Drop to 128 only if your inputs are very short and you need maximum throughput.\n",
    "-\tStabilize short-length runs (especially 128):\n",
    "-       \tReduce LR slightly (e.g., 1e-5 instead of ~1.7e-5).\n",
    "-       \tAdd/keep max_grad_norm=1.0.\n",
    "-       \tKeep masked-tokens per update roughly constant across lengths: if you halve max_len, double effective batch (via per_device_* and/or gradient_accumulation_steps) or slightly increase mlm_probability (e.g., 0.15 → 0.20) so each step still trains on a similar number of masked words.\n",
    "\n",
    "-\tDecide by data length: If ≥90% of your samples are ≤130 tokens, 128 may be fine. Otherwise, 256 is safer. (You can quickly histogram tokenized lengths once to confirm.)\n",
    "-\tDownstream is the real test: WWM’s benefit usually shows up on word-level tasks (NER, QA, classification). If downstream metrics are flat between 128 and 256, pick the faster one; if 256 wins, stick with it.\n",
    "\n",
    "### About the warning\n",
    "- The “tokenizer is deprecated… use processing_class” warning is forward-looking for Transformers v5; it’s safe to ignore for now. No action needed unless you upgrade to v5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252bd947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9: torch.Size([1, 12, 30522]) logits\n",
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 6522.84 examples/s]\n",
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5731, 'grad_norm': 0.5677264928817749, 'learning_rate': 4.2461538461538465e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.34220090508461, 'eval_runtime': 363.8509, 'eval_samples_per_second': 2.748, 'eval_steps_per_second': 0.344, 'epoch': 0.8}\n",
      "{'loss': 0.335, 'grad_norm': 0.43416109681129456, 'learning_rate': 2.7076923076923078e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.2606508731842041, 'eval_runtime': 386.6311, 'eval_samples_per_second': 2.586, 'eval_steps_per_second': 0.323, 'epoch': 1.6}\n",
      "{'loss': 0.2919, 'grad_norm': 0.37298446893692017, 'learning_rate': 1.1692307692307693e-05, 'epoch': 2.4}\n",
      "{'eval_loss': 0.22133523225784302, 'eval_runtime': 372.1285, 'eval_samples_per_second': 2.687, 'eval_steps_per_second': 0.336, 'epoch': 2.4}\n",
      "{'train_runtime': 3250.5533, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.115, 'train_loss': 0.3720854746500651, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.3720854746500651, metrics={'train_runtime': 3250.5533, 'train_samples_per_second': 0.923, 'train_steps_per_second': 0.115, 'train_loss': 0.3720854746500651, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of bert model for sample input\n",
    "sample_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = bert_tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "outputs = bert_model(**inputs)\n",
    "print(f\"Sample {i}: {outputs.logits.shape} logits\")       \n",
    "\n",
    "# Freeze the BERT model and train a classifier on top\n",
    "# load a news sentiment classification dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=384, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "# Use the trained model as a feature extractor and add a classification head\n",
    "def get_news_classifier_model():\n",
    "    \"\"\"\n",
    "    load the trained BERT model and add a classification head\n",
    "    \"\"\"\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"./bert-mlm\") # Load the trained BERT model\n",
    "    # Freeze the BERT model\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Add a classification head\n",
    "    model.classifier = torch.nn.Linear(model.config.hidden_size, 4)  # 4 classes for AG News\n",
    "    return model\n",
    "\n",
    "news_classifier_model = get_news_classifier_model()\n",
    "# Training arguments for news classifier\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm=False),\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cbe49",
   "metadata": {},
   "source": [
    "# Obervations of Print output of bert model for sample input\n",
    "### For varying Max Length with constant Sample size of 1000.\n",
    "\n",
    "#### Story about how max_length affects both speed and fit for your BERT(MLM) setup on a 1,000-sample news dataset.\n",
    "\n",
    "### Consolidated (last eval in each run)\n",
    "#### max_len\tTrain runtime\tSteps/s\tEval loss (≈quality)\tPseudo-PPL exp(eval_loss)\tSpeedup vs 512\n",
    "- 512\t85m 56s\t0.073\t0.22071\t1.247\t1.00×\n",
    "- 384\t54m 14s\t0.115\t0.22134\t1.248\t1.59×\n",
    "- 128\t20m 13.7s\t0.310\t0.22178\t1.249\t4.26×\n",
    "\n",
    "The differences in eval loss are tiny: worst–best gap ≈ +0.00107 (≈ +0.49%). Meanwhile, speed jumps are big.\n",
    "\n",
    "Lessons learnt from this\n",
    "1.\tSpeed scales steeply with shorter sequences.\n",
    "- Reducing max_length from 512 → 128 gives you ~4.3× faster training (0.073 → 0.31 steps/s). This is the classic O(n²) attention cost showing up.\n",
    "2.\tQuality (eval loss) barely changes across lengths for this dataset.\n",
    "- All three runs converge to ~0.221–0.222 eval loss (pseudo-perplexity ~1.247–1.249). That strongly suggests most of your examples are short enough that 128 tokens already cover the useful context for MLM.\n",
    "3.\tSo the best default for this corpus is small.\n",
    "- If your goal is fast domain adaptation, max_length=128 (or 256 if you want extra buffer) is a great trade-off: ~4× faster for ~same eval loss.\n",
    "4.\tIf you later add longer documents, length will matter.\n",
    "- Expect longer max_length to help only when your inputs regularly exceed the shorter cap. Right now, the near-identical eval losses imply little truncation pain.\n",
    "5.\tThe logits shape you printed ([1, 12, 30522]) is expected.\n",
    "- It’s [batch, seq_len, vocab]. Your sample had 12 tokens after tokenization; that’s independent of global max_length if the example is short.\n",
    "6.\tYou’re training on CPU.\n",
    "- The pin-memory warning and modest throughput show no CUDA device is used. A GPU will multiply speeds across all max_length values (the relative speedups will still favor shorter lengths).\n",
    "\n",
    "Quick wins / next steps\n",
    "-\tPick 128 (or 256) for routine runs; use 384/512 only when you truly need it.\n",
    "-\tMatch masked-tokens per step when you compare lengths: keep batch_size × max_length × mlm_prob roughly constant (use grad accumulation) so gradients are comparable.\n",
    "-\tAdd a length histogram once: if, say, 90% of samples are ≤120 tokens, lock in 128 confidently.\n",
    "-\tStability knobs: set max_grad_norm=1.0, dataloader_num_workers=2–4, and dataloader_pin_memory=False on CPU.\n",
    "-\tDownstream check (most important): show that the MLM adaptation improves a task you care about (classification/NER). If downstream metrics are flat across 128 vs 512, that seals the case for shorter max_len.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e79900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News dataset loaded with 1000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PuttarajuS\\AppData\\Local\\Temp\\ipykernel_23704\\2350930871.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  news_trainer = Trainer(\n",
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1395, 'grad_norm': 4.596009254455566, 'learning_rate': 4.831034482758621e-05, 'epoch': 0.2}\n",
      "{'eval_loss': 0.6283857822418213, 'eval_runtime': 429.5296, 'eval_samples_per_second': 2.328, 'eval_steps_per_second': 1.164, 'epoch': 0.2}\n",
      "{'loss': 0.5949, 'grad_norm': 0.06072872877120972, 'learning_rate': 4.486206896551725e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 0.7693285942077637, 'eval_runtime': 630.8441, 'eval_samples_per_second': 1.585, 'eval_steps_per_second': 0.793, 'epoch': 0.4}\n",
      "{'loss': 0.8522, 'grad_norm': 0.1986725926399231, 'learning_rate': 4.141379310344828e-05, 'epoch': 0.6}\n",
      "{'eval_loss': 0.653421938419342, 'eval_runtime': 675.1535, 'eval_samples_per_second': 1.481, 'eval_steps_per_second': 0.741, 'epoch': 0.6}\n",
      "{'loss': 0.5952, 'grad_norm': 0.04177295044064522, 'learning_rate': 3.796551724137931e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 0.5812411904335022, 'eval_runtime': 487.0637, 'eval_samples_per_second': 2.053, 'eval_steps_per_second': 1.027, 'epoch': 0.8}\n",
      "{'loss': 0.8056, 'grad_norm': 0.11734509468078613, 'learning_rate': 3.451724137931035e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.40492284297943115, 'eval_runtime': 432.3254, 'eval_samples_per_second': 2.313, 'eval_steps_per_second': 1.157, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3971, 'grad_norm': 2.11323881149292, 'learning_rate': 3.106896551724138e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 0.3556310534477234, 'eval_runtime': 447.948, 'eval_samples_per_second': 2.232, 'eval_steps_per_second': 1.116, 'epoch': 1.2}\n",
      "{'loss': 0.444, 'grad_norm': 90.89733123779297, 'learning_rate': 2.7620689655172417e-05, 'epoch': 1.4}\n",
      "{'eval_loss': 0.29820308089256287, 'eval_runtime': 472.0438, 'eval_samples_per_second': 2.118, 'eval_steps_per_second': 1.059, 'epoch': 1.4}\n",
      "{'loss': 0.4282, 'grad_norm': 0.8346530199050903, 'learning_rate': 2.417241379310345e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 0.35103848576545715, 'eval_runtime': 489.5177, 'eval_samples_per_second': 2.043, 'eval_steps_per_second': 1.021, 'epoch': 1.6}\n",
      "{'loss': 0.4522, 'grad_norm': 0.06494677066802979, 'learning_rate': 2.0724137931034484e-05, 'epoch': 1.8}\n",
      "{'eval_loss': 0.3025656044483185, 'eval_runtime': 484.9571, 'eval_samples_per_second': 2.062, 'eval_steps_per_second': 1.031, 'epoch': 1.8}\n",
      "{'loss': 0.4513, 'grad_norm': 0.1440640687942505, 'learning_rate': 1.727586206896552e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.182872012257576, 'eval_runtime': 546.5846, 'eval_samples_per_second': 1.83, 'eval_steps_per_second': 0.915, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Downloads\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1068, 'grad_norm': 0.040200404822826385, 'learning_rate': 1.3827586206896554e-05, 'epoch': 2.2}\n",
      "{'eval_loss': 0.21806420385837555, 'eval_runtime': 600.0138, 'eval_samples_per_second': 1.667, 'eval_steps_per_second': 0.833, 'epoch': 2.2}\n",
      "{'loss': 0.3062, 'grad_norm': 40.25448226928711, 'learning_rate': 1.0379310344827587e-05, 'epoch': 2.4}\n",
      "{'eval_loss': 0.14462047815322876, 'eval_runtime': 524.1228, 'eval_samples_per_second': 1.908, 'eval_steps_per_second': 0.954, 'epoch': 2.4}\n",
      "{'loss': 0.2372, 'grad_norm': 0.03175517916679382, 'learning_rate': 6.931034482758621e-06, 'epoch': 2.6}\n",
      "{'eval_loss': 0.11329276859760284, 'eval_runtime': 503.1102, 'eval_samples_per_second': 1.988, 'eval_steps_per_second': 0.994, 'epoch': 2.6}\n",
      "{'loss': 0.2202, 'grad_norm': 25.70432472229004, 'learning_rate': 3.4827586206896552e-06, 'epoch': 2.8}\n",
      "{'eval_loss': 0.100176602602005, 'eval_runtime': 493.4204, 'eval_samples_per_second': 2.027, 'eval_steps_per_second': 1.013, 'epoch': 2.8}\n",
      "{'loss': 0.0973, 'grad_norm': 0.05647110193967819, 'learning_rate': 3.448275862068965e-08, 'epoch': 3.0}\n",
      "{'eval_loss': 0.09249687939882278, 'eval_runtime': 604.0983, 'eval_samples_per_second': 1.655, 'eval_steps_per_second': 0.828, 'epoch': 3.0}\n",
      "{'train_runtime': 15262.7068, 'train_samples_per_second': 0.197, 'train_steps_per_second': 0.098, 'train_loss': 0.4751887887318929, 'epoch': 3.0}\n",
      "Sample news input: Breaking news: The stock market crashes as investors panic.\n",
      "Predicted class: 2\n",
      "Available classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Print sample output of the news classifier\n",
    "# The previous approach used AutoModelForMaskedLM, which is not suitable for classification.\n",
    "# Instead, use AutoModelForSequenceClassification for news classification.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a BERT model with a classification head\n",
    "news_classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=4\n",
    ").to(device)\n",
    "\n",
    "# Training arguments for news classifier remain the same\n",
    "news_training_args = TrainingArguments(\n",
    "    output_dir=\"./news-classifier\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Load the AG News dataset\n",
    "from datasets import load_dataset\n",
    "news_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(\"News dataset loaded with\", len(news_dataset), \"samples.\")\n",
    "\n",
    "# Preprocess the dataset for BERT\n",
    "def preprocess_news(examples):\n",
    "    model_inputs = bert_tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_news = news_dataset.map(preprocess_news, batched=True, remove_columns=news_dataset.column_names)\n",
    "\n",
    "# Create Trainer for news classifier\n",
    "news_trainer = Trainer(\n",
    "    model=news_classifier_model,\n",
    "    args=news_training_args,\n",
    "    train_dataset=train_news,\n",
    "    eval_dataset=train_news,  # For simplicity, using the same dataset for eval\n",
    "    tokenizer=bert_tokenizer,\n",
    ")\n",
    "# Uncomment the line below to train the news classifier\n",
    "news_trainer.train()\n",
    "\n",
    "# Print sample output of the news classifier\n",
    "sample_news_input = \"Breaking news: The stock market crashes as investors panic.\"\n",
    "inputs = bert_tokenizer(sample_news_input, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = news_classifier_model(**inputs)\n",
    "pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "print(f\"Sample news input: {sample_news_input}\")\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(\"Available classes:\", news_dataset.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573d1c9",
   "metadata": {},
   "source": [
    "News classifier run.\n",
    "\n",
    "1. Dataset & setup\n",
    "\n",
    "News dataset loaded with 1000 samples.\n",
    "\n",
    "That’s fairly small for training a Transformer classifier, so we should expect the model to overfit quickly (training loss going down very fast compared to evaluation loss).\n",
    "\n",
    "Warnings: tokenizer deprecation → harmless, just update to processing_class in the future.\n",
    "\n",
    "pin_memory warning → you’re training on CPU (no GPU found), so pinned memory won’t be used. Also harmless.\n",
    "\n",
    "2. Training loss vs. evaluation loss\n",
    "\n",
    "We have a mix of train loss and eval_loss across epochs:\n",
    "\n",
    "Epoch\tTrain Loss\tEval Loss\tWhat it means\n",
    "0.2\t1.14\t0.63\tVery early, model starting to learn but not stable yet\n",
    "0.4\t0.59\t0.77\tTrain loss ↓, but eval loss ↑ → slight overfitting spike\n",
    "0.8\t0.59\t0.58\tBalanced, good generalization\n",
    "1.0\t0.81\t0.40\tEval improves → model is learning to classify\n",
    "1.4\t0.44\t0.30\tBoth improving, stronger fit\n",
    "2.0\t0.45\t0.18\tEval loss keeps dropping — generalization is good\n",
    "2.6\t0.24\t0.11\tExcellent evaluation performance\n",
    "3.0\t0.097\t0.092\tTrain & eval converge → model is very confident and consistent\n",
    "\n",
    " Inference: The model improved steadily, and by epoch ~3, it has very low loss on both training and evaluation — this suggests it can classify news articles into your categories quite accurately.\n",
    "\n",
    "3. Grad norm & learning rate\n",
    "Grad_norm is the size of your gradient updates.\n",
    "\n",
    "Most of the time it’s small (<1), but occasionally you see spikes (e.g., 90 at epoch 1.4, 40 at epoch 2.4).\n",
    "\n",
    "Spikes suggest certain batches had much larger gradient updates — can be due to noisy data or outlier samples. If training is stable (which it seems to be), you can ignore them.\n",
    "\n",
    "Learning_rate: decayed from 4.8e-5 → 3.4e-8 by epoch 3.\n",
    "\n",
    "This is standard with schedulers: start big, shrink gradually.\n",
    "\n",
    "By the end, updates are tiny, so the model is just fine-tuning around a stable point.\n",
    "\n",
    "4. Final training summary\n",
    "- 'train_runtime': 15262s (~4.2 hours on CPU),\n",
    "- 'train_loss': 0.475,\n",
    "- 'eval_loss' (final): ~0.092\n",
    "\n",
    "\n",
    "Runtime is long because CPU training is slow.\n",
    "\n",
    "Final losses show the model learned very well despite the small dataset.\n",
    "\n",
    "5. Sample prediction\n",
    "- Input: “Breaking news: The stock market crashes as investors panic.”\n",
    "- Predicted class: 2\n",
    "- Available classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "So:\n",
    "- Class 2 = Business → correct, since stock market crash is a business/economy story.\n",
    "- This shows the model generalized well enough to classify unseen text.\n",
    "\n",
    "### Inference:\n",
    "- The model trained successfully — losses dropped steadily, and evaluation loss reached a very low number.\n",
    "- Generalization is good — no major overfitting, eval loss kept improving alongside training.\n",
    "- Classifier works as expected — predicted “Business” for a finance-related story.\n",
    "- Training on CPU is slow — moving to GPU would speed things up a lot.\n",
    "- Future tweaks —\n",
    "-           Watch out for occasional high grad_norm spikes (gradient clipping could help).\n",
    "-           Dataset size (1000) is small; with more data, performance would be more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb6d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from seaborn) (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71924d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[207   2   2   1]\n",
      " [  0 142   0   0]\n",
      " [  0   0 164  10]\n",
      " [  0   0   5 467]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for the entire training set\n",
    "predictions = []\n",
    "for i in range(0, len(train_news), 32):  # Batch size of 32\n",
    "    batch = train_news[i:i+32]\n",
    "    # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "    # You need to decode input_ids to get the original text if you want to print or use text\n",
    "    # For inference, use input_ids, attention_mask, etc.\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = news_classifier_model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "# Get true labels\n",
    "true_labels = np.array(batch[\"labels\"] for batch in train_news)  # This line should be fixed\n",
    "# Instead, collect all labels from train_news:\n",
    "true_labels = np.array(train_news[\"labels\"])\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=news_dataset.features[\"label\"].names,\n",
    "            yticklabels=news_dataset.features[\"label\"].names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for News Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1648c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from rouge-score) (2.3.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\puttarajus\\downloads\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk rouge-score absl-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0094341",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ee4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation Report: {\n",
      "  \"0\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 0.9523809523809523,\n",
      "    \"f1-score\": 0.975609756097561,\n",
      "    \"support\": 21.0\n",
      "  },\n",
      "  \"1\": {\n",
      "    \"precision\": 0.9411764705882353,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 0.9696969696969697,\n",
      "    \"support\": 16.0\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 18.0\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"precision\": 1.0,\n",
      "    \"recall\": 1.0,\n",
      "    \"f1-score\": 1.0,\n",
      "    \"support\": 45.0\n",
      "  },\n",
      "  \"accuracy\": 0.99,\n",
      "  \"macro avg\": {\n",
      "    \"precision\": 0.9852941176470589,\n",
      "    \"recall\": 0.9880952380952381,\n",
      "    \"f1-score\": 0.9863266814486327,\n",
      "    \"support\": 100.0\n",
      "  },\n",
      "  \"weighted avg\": {\n",
      "    \"precision\": 0.9905882352941177,\n",
      "    \"recall\": 0.99,\n",
      "    \"f1-score\": 0.9900295639320029,\n",
      "    \"support\": 100.0\n",
      "  }\n",
      "}\n",
      "[{'score': 0.08737759292125702, 'token': 4010, 'token_str': 'warm', 'sequence': 'the weather today is warm.'}, {'score': 0.07628422230482101, 'token': 3147, 'token_str': 'cold', 'sequence': 'the weather today is cold.'}, {'score': 0.0671607255935669, 'token': 2204, 'token_str': 'good', 'sequence': 'the weather today is good.'}, {'score': 0.04326416179537773, 'token': 2919, 'token_str': 'bad', 'sequence': 'the weather today is bad.'}, {'score': 0.04188793897628784, 'token': 11559, 'token_str': 'sunny', 'sequence': 'the weather today is sunny.'}]\n"
     ]
    }
   ],
   "source": [
    "# BERT evaluation: compute F1 score - sklearn classification report\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate_bert(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(0, len(dataset), 32):  # Batch size of 32\n",
    "        batch = dataset[i:i+32]\n",
    "        # batch is a dict with keys: input_ids, token_type_ids, attention_mask, labels\n",
    "        # You need to decode input_ids to get the original text if you want to print or use text\n",
    "        # For inference, use input_ids, attention_mask, etc.\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = news_classifier_model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    true_labels = np.array(dataset[\"labels\"])\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    return report\n",
    "# Evaluate on random 100 samples\n",
    "bert_eval_results = evaluate_bert(news_classifier_model, train_news.shuffle(seed=42).select(range(100)))\n",
    "import json\n",
    "print(\"BERT Evaluation Report:\", json.dumps(bert_eval_results, indent=2))\n",
    "\n",
    "# Demonstrate fill-mask prediction with BERT\n",
    "from transformers import pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "sentence = \"The weather today is [MASK].\"\n",
    "print(fill_mask(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0092999",
   "metadata": {},
   "source": [
    "## Evaluation BERT \n",
    "### BERT (Encoder-only)\n",
    "####\tClassification metrics:\n",
    "-\tAccuracy ≈ 99%.\n",
    "-\tMacro F1 = 0.986 (very strong across all classes).\n",
    "-\tNear-perfect precision/recall for multiple categories.\n",
    "####\tExample inference: Given input “the weather today is …”, BERT assigns probabilities to possible tokens. Top candidates include warm, cold, good, bad, sunny with confidence scores.\n",
    "####\tInference:\n",
    "-\tBERT excels at classification and token prediction tasks, not summarization.\n",
    "-\tIts strength lies in understanding context for labels.\n",
    "-\tExtremely strong accuracy here shows that for discriminative tasks, BERT outperforms both GPT-2 and T5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
